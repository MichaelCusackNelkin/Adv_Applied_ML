{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SOqMlRibziB"
      },
      "source": [
        "# DATA 410 Lecture 10 - Spring 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDb8bGUrbV3Y"
      },
      "source": [
        "<font face=\"Chalkboard\" color=\"darkgreen\" size=10> Multivariate Models for Regression</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1NhfDMt1bVyx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.linalg import lstsq\n",
        "from scipy.sparse.linalg import lsmr\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d, griddata, LinearNDInterpolator, NearestNDInterpolator\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold, train_test_split as tts\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJmTp3nUym6A"
      },
      "source": [
        "## Gradient Boosting\n",
        "\n",
        "Assume you have an regressor $F$ and, for the observation $x_i$ we make the prediction $F(x_i)$. To improve the predictions, we can regard $F$ as a 'weak learner' and therefore train a decision tree (we can call it $h$) where its predictions are $y_i-F(x_i)$. Thus, there are increased chances that the new regressor\n",
        "\n",
        "$$\\large F + h$$ \n",
        "\n",
        "is better than the old one, $F.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hy7vVKWy_ijS"
      },
      "outputs": [],
      "source": [
        "# import libraries for creating a neural network\n",
        "# imports for creating a Neural Network\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.optimizers import Adam # they recently updated Tensorflow\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oc9b_REyAEI_"
      },
      "outputs": [],
      "source": [
        "# Tricubic Kernel\n",
        "def Tricubic(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,70/81*(1-d**3)**3)\n",
        "\n",
        "# Quartic Kernel\n",
        "def Quartic(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,15/16*(1-d**2)**2)\n",
        "\n",
        "# Epanechnikov Kernel\n",
        "def Epanechnikov(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,3/4*(1-d**2)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iudc4aEGAXBw"
      },
      "outputs": [],
      "source": [
        "#Defining the kernel local regression model\n",
        "\n",
        "def lw_reg(X, y, xnew, kern, tau, intercept):\n",
        "    # tau is called bandwidth K((x-x[i])/(2*tau))\n",
        "    n = len(X) # the number of observations\n",
        "    yest = np.zeros(n)\n",
        "\n",
        "    if len(y.shape)==1: # here we make column vectors\n",
        "      y = y.reshape(-1,1)\n",
        "\n",
        "    if len(X.shape)==1:\n",
        "      X = X.reshape(-1,1)\n",
        "    \n",
        "    if intercept:\n",
        "      X1 = np.column_stack([np.ones((len(X),1)),X])\n",
        "    else:\n",
        "      X1 = X\n",
        "\n",
        "    w = np.array([kern((X - X[i])/(2*tau)) for i in range(n)]) # here we compute n vectors of weights\n",
        "\n",
        "    #Looping through all X-points\n",
        "    for i in range(n):          \n",
        "        W = np.diag(w[:,i])\n",
        "        b = np.transpose(X1).dot(W).dot(y)\n",
        "        A = np.transpose(X1).dot(W).dot(X1)\n",
        "        #A = A + 0.001*np.eye(X1.shape[1]) # if we want L2 regularization\n",
        "        #theta = linalg.solve(A, b) # A*theta = b\n",
        "        beta, res, rnk, s = lstsq(A, b)\n",
        "        yest[i] = np.dot(X1[i],beta)\n",
        "    if X.shape[1]==1:\n",
        "      f = interp1d(X.flatten(),yest,fill_value='extrapolate')\n",
        "    else:\n",
        "      f = LinearNDInterpolator(X, yest)\n",
        "    output = f(xnew) # the output may have NaN's where the data points from xnew are outside the convex hull of X\n",
        "    if sum(np.isnan(output))>0:\n",
        "      g = NearestNDInterpolator(X,y.ravel()) \n",
        "      # output[np.isnan(output)] = g(X[np.isnan(output)])\n",
        "      output[np.isnan(output)] = g(xnew[np.isnan(output)])\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pVfSPIBqAXEB"
      },
      "outputs": [],
      "source": [
        "def boosted_lwr(X, y, xtest, kern, tau, intercept):\n",
        "  # we need decision trees\n",
        "  # for training the boosted method we use X and y\n",
        "  Fx = lw_reg(X,y,X,kern,tau,intercept) # we need this for training the Decision Tree\n",
        "  # Now train the Decision Tree on y_i - F(x_i)\n",
        "  grad_neg = y - Fx #Resdiduals are the negative gradient for MSE Loss function\n",
        "  tree_model = DecisionTreeRegressor(max_depth=2, random_state=123)\n",
        "  tree_model.fit(X,grad_neg)\n",
        "  output = lw_reg(X,y,xtest,kern,tau,intercept) + tree_model.predict(xtest)\n",
        "  return output "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCGLoGtiAGAb"
      },
      "source": [
        "## Create a \"boosted\" (in the sense of Gradient Boosting) version of the Locally Weighted Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BADFeJnTAXIV"
      },
      "outputs": [],
      "source": [
        "cars = pd.read_csv('Data/cars.csv')\n",
        "\n",
        "X = cars[['ENG','CYL','WGT']].values\n",
        "y = cars['MPG'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "A8diPd2LEaJh"
      },
      "outputs": [],
      "source": [
        "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
        "scale = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQZl_-2lAXKW",
        "outputId": "b227f9bb-0d99-4d86-bb6a-bdeb2894ff9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Cross-validated Mean Squared Error for LWR is : 17.090059925939794\n",
            "The Cross-validated Mean Squared Error for BLWR is : 17.64648807322922\n",
            "The Cross-validated Mean Squared Error for RF is : 17.77482685909969\n"
          ]
        }
      ],
      "source": [
        "mse_blwr = []\n",
        "mse_lwr = []\n",
        "mse_rf = []\n",
        "mse_nn = []\n",
        "\n",
        "states = [1234, 2345, 3456, 4567, 5678]\n",
        "\n",
        "total_mse_blwr = []\n",
        "total_mse_lwr = []\n",
        "total_mse_rf = []\n",
        "total_mse_nn = []\n",
        "\n",
        "# this is the Cross-Validation Loop\n",
        "for state in states:\n",
        "  total_mse_blwr = []\n",
        "  total_mse_lwr = []\n",
        "  total_mse_rf = []\n",
        "  total_mse_nn = []\n",
        "\n",
        "  kf = KFold(n_splits=5,shuffle=True,random_state=state)\n",
        "  for idxtrain, idxtest in kf.split(X):\n",
        "    xtrain = X[idxtrain]\n",
        "    ytrain = y[idxtrain]\n",
        "    ytest = y[idxtest]\n",
        "    xtest = X[idxtest]\n",
        "    xtrain = scale.fit_transform(xtrain)\n",
        "    xtest = scale.transform(xtest)\n",
        "\n",
        "    yhat_lwr = lw_reg(xtrain,ytrain, xtest,Tricubic,tau=0.5,intercept=True)\n",
        "    yhat_blwr = boosted_lwr(xtrain,ytrain, xtest,Tricubic,tau=1,intercept=True)\n",
        "\n",
        "    model_rf = RandomForestRegressor(n_estimators=100, max_depth=4, random_state = 123)\n",
        "    model_rf.fit(xtrain,ytrain)\n",
        "    yhat_rf = model_rf.predict(xtest)\n",
        "\n",
        "    #model_nn.fit(xtrain,ytrain,validation_split=0.3, epochs=500, batch_size=20, verbose=0, callbacks=[es])\n",
        "    #yhat_nn = model_nn.predict(xtest)\n",
        "\n",
        "    mse_lwr.append(mse(ytest,yhat_lwr))\n",
        "    mse_blwr.append(mse(ytest,yhat_blwr))\n",
        "    mse_rf.append(mse(ytest,yhat_rf))\n",
        "    #mse_nn.append(mse(ytest,yhat_nn))\n",
        "  \n",
        "  total_mse_lwr.append(np.mean(mse_lwr))\n",
        "  total_mse_blwr.append(np.mean(mse_blwr))\n",
        "  total_mse_rf.append(np.mean(mse_rf))\n",
        "  #total_mse_nn.append(np.mean(mse_nn))\n",
        "\n",
        "mse_blwr = []\n",
        "mse_lwr = []\n",
        "mse_rf = []\n",
        "mse_nn = []\n",
        "  \n",
        "  \n",
        "\n",
        "print('The Cross-validated Mean Squared Error for LWR is : '+str(np.mean(total_mse_lwr)))\n",
        "print('The Cross-validated Mean Squared Error for BLWR is : '+str(np.mean(total_mse_blwr)))\n",
        "print('The Cross-validated Mean Squared Error for RF is : '+str(np.mean(total_mse_rf)))\n",
        "#print('The Cross-validated Mean Squared Error for NN is : '+str(np.mean(total_mse_nn)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_uDLN75b2qH"
      },
      "source": [
        "## Here are the predictions we made for the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUbUsyGh-nRw",
        "outputId": "3663bd78-ad17-4dbd-90ca-bf4ca8a8838b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model_nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22844/4118653347.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0myhat_nn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmae_nn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat_nn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model_nn' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "model_nn.fit(xtrain,ytrain,validation_split=0.3, epochs=500, batch_size=20, verbose=0, callbacks=[es])\n",
        "yhat_nn = model.predict(dat_test[:,:-1])\n",
        "mae_nn = mean_absolute_error(dat_test[:,-1], yhat_nn)\n",
        "print(\"MAE Neural Network = ${:,.2f}\".format(1000*mae_nn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-fji3Nt-owj"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,9))\n",
        "ax.set_xlim(3, 9)\n",
        "ax.set_ylim(0, 51)\n",
        "ax.scatter(x=df['rooms'], y=df['cmedv'],s=25)\n",
        "ax.plot(X_test, lm.predict(X_test), color='red',label='Linear Regression')\n",
        "ax.plot(dat_test[:,0], yhat_nn, color='lightgreen',lw=2.5,label='Neural Network')\n",
        "ax.plot(dat_test[:,0], model_lowess(dat_train,dat_test,Epanechnikov,0.53), color='orange',lw=2.5,label='Kernel Weighted Regression')\n",
        "ax.set_xlabel('Number of Rooms',fontsize=16,color='navy')\n",
        "ax.set_ylabel('House Price (Thousands of Dollars)',fontsize=16,color='navy')\n",
        "ax.set_title('Boston Housing Prices',fontsize=16,color='purple')\n",
        "ax.grid(b=True,which='major', color ='grey', linestyle='-', alpha=0.8)\n",
        "ax.grid(b=True,which='minor', color ='grey', linestyle='--', alpha=0.2)\n",
        "ax.minorticks_on()\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fObIlRhKJFy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqzyt7BARDCl"
      },
      "outputs": [],
      "source": [
        "kf = KFold(n_splits=10, shuffle=True, random_state=1693)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWdGHA9TRHVw",
        "outputId": "ac7dbdaf-d274-45fb-8617-742ad3241d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validated MAE Linear Regression = $4,447.94\n"
          ]
        }
      ],
      "source": [
        "mae_lm = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  X_train = dat[idxtrain,0]\n",
        "  y_train = dat[idxtrain,1]\n",
        "  X_test  = dat[idxtest,0]\n",
        "  y_test = dat[idxtest,1]\n",
        "  lm.fit(X_train.reshape(-1,1),y_train)\n",
        "  yhat_lm = lm.predict(X_test.reshape(-1,1))\n",
        "  mae_lm.append(mean_absolute_error(y_test, yhat_lm))\n",
        "print(\"Validated MAE Linear Regression = ${:,.2f}\".format(1000*np.mean(mae_lm)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1yCP16cgFSz",
        "outputId": "68768f49-ad4a-4f61-ed46-e686cdddd663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validated MAE Local Kernel Regression = $4,090.03\n",
            "Validated MAE Local Kernel Regression = $4,090.03\n",
            "Validated MAE Local Kernel Regression = $4,090.03\n",
            "1 loop, best of 3: 531 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "\n",
        "mae_lk = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  dat_test = dat[idxtest,:]\n",
        "  y_test = dat_test[np.argsort(dat_test[:, 0]),1]\n",
        "  yhat_lk = model_lowess(dat[idxtrain,:],dat[idxtest,:],Gaussian,0.15)\n",
        "  mae_lk.append(mean_absolute_error(y_test, yhat_lk))\n",
        "print(\"Validated MAE Local Kernel Regression = ${:,.2f}\".format(1000*np.mean(mae_lk)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_CUmGMZR0Tk",
        "outputId": "456934f3-024b-4d5e-d6e8-a6f81407beed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00501: early stopping\n",
            "Epoch 00506: early stopping\n",
            "Epoch 00539: early stopping\n",
            "Epoch 00800: early stopping\n",
            "Epoch 00672: early stopping\n",
            "Epoch 00689: early stopping\n",
            "Epoch 00551: early stopping\n",
            "Validated MAE Neural Network Regression = $5,108.02\n"
          ]
        }
      ],
      "source": [
        "#%%timeit -n 1\n",
        "\n",
        "mae_nn = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  X_train = dat[idxtrain,0]\n",
        "  y_train = dat[idxtrain,1]\n",
        "  X_test  = dat[idxtest,0]\n",
        "  y_test = dat[idxtest,1]\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=500)\n",
        "  model.fit(X_train.reshape(-1,1),y_train,validation_split=0.3, epochs=1000, batch_size=100, verbose=0, callbacks=[es])\n",
        "  yhat_nn = model.predict(X_test.reshape(-1,1))\n",
        "  mae_nn.append(mean_absolute_error(y_test, yhat_nn))\n",
        "print(\"Validated MAE Neural Network Regression = ${:,.2f}\".format(1000*np.mean(mae_nn)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mom_T9x8bUHl"
      },
      "source": [
        "## XGBoost\n",
        "\n",
        "The method is related to Random Forest\n",
        "\n",
        "https://towardsdatascience.com/xgboost-python-example-42777d01001e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Z3iQle1ER-cI"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OwEh4jc3wKyw"
      },
      "outputs": [],
      "source": [
        "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO9TMxFBwcen",
        "outputId": "b346e654-36ba-4bf1-ef71-ec11c0806790"
      },
      "outputs": [
        {
          "ename": "XGBoostError",
          "evalue": "[14:47:07] C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/data/data.cc:592: Check failed: labels_.Size() == num_row_ (313 vs. 939) : Size of labels must equal to number of rows.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22844/3897221129.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-n 1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\nmae_xgb = []\\n\\nfor idxtrain, idxtest in kf.split(X):\\n  X_train = X[idxtrain]\\n  y_train = y[idxtrain]\\n  X_test  = X[idxtest]\\n  y_test = y[idxtest]\\n  model_xgb.fit(X_train.reshape(-1,1),y_train)\\n  yhat_xgb = model_xgb.predict(X_test.reshape(-1,1))\\n  mae_xgb.append(mean_absolute_error(y_test, yhat_xgb))\\nprint(\"Validated MAE XGBoost Regression = ${:,.2f}\".format(1000*np.mean(mae_xgb)))\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2401\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2402\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2403\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2404\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\decorator.py\u001b[0m in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1171\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m         \u001b[0mall_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m         \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[0mworst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(self, repeat, number)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m--> 188\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    189\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1681\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m                                                     dtrain.handle))\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \"\"\"\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mXGBoostError\u001b[0m: [14:47:07] C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/data/data.cc:592: Check failed: labels_.Size() == num_row_ (313 vs. 939) : Size of labels must equal to number of rows."
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "\n",
        "mae_xgb = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(X):\n",
        "  X_train = X[idxtrain]\n",
        "  y_train = y[idxtrain]\n",
        "  X_test  = X[idxtest]\n",
        "  y_test = y[idxtest]\n",
        "  model_xgb.fit(X_train.reshape(-1,1),y_train)\n",
        "  yhat_xgb = model_xgb.predict(X_test.reshape(-1,1))\n",
        "  mae_xgb.append(mean_absolute_error(y_test, yhat_xgb))\n",
        "print(\"Validated MAE XGBoost Regression = ${:,.2f}\".format(1000*np.mean(mae_xgb)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk4DCf7-rzEy"
      },
      "outputs": [],
      "source": [
        "cstring = 'c'*364"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQyF8aP3r6_"
      },
      "source": [
        "## Using Kernel Regression from StatsModels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZhrdVXTs9mL"
      },
      "outputs": [],
      "source": [
        "hello = 'c'*dat_train_poly.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-Qm9aaFVs9xW",
        "outputId": "2806c44a-981f-49bc-b4ab-9a8f7566f109"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc'"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP_wvqM9thfi"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQTBZ5cttoZF"
      },
      "outputs": [],
      "source": [
        "poly = PolynomialFeatures(degree=2)\n",
        "dat_train_poly = poly.fit_transform(dat_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obBU-bGuwsKq"
      },
      "outputs": [],
      "source": [
        "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
        "\n",
        "\n",
        "model_KernReg = KernelReg(endog=dat_train[:,-1],exog=dat_train_poly,var_type=hello,ckertype='gaussian')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0fq_s0_CLMI"
      },
      "outputs": [],
      "source": [
        "yhat_sm_test, y_std = model_KernReg.fit(dat_test[:,:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geqRpN8siZ1W",
        "outputId": "0842b17b-6a46-469e-e4b9-e731221a5ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE StatsModels Kernel Regression = $2,854.57\n"
          ]
        }
      ],
      "source": [
        "mae_sm = mean_absolute_error(dat_test[:,-1], yhat_sm_test)\n",
        "print(\"MAE StatsModels Kernel Regression = ${:,.2f}\".format(1000*mae_sm))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "DATA 410  Lecture 10 - Spring 2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
