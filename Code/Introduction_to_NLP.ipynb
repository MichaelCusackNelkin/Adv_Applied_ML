{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47zhKygt16DR"
      },
      "source": [
        "##<font color='navy' face='Helvetica' size=12pt>Introduction to Natural Language Processing</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeSccFn3kPfE"
      },
      "source": [
        "<font color='crimson'>**Objective:** use speech and words along with computer run algorithms.\n",
        "\n",
        "<span style=\"font-family:Calibri; color:darkblue; font-size:18pt;\">Examples of projects/research with NLP:</span>\n",
        "\n",
        "<font color='blue'>*Sentiment Analysis*</font> - How positive or negative is text about a topic? \n",
        "\n",
        "<font color='blue'>*Prediction*</font> - What genres should Netflix classify a movie as to maximize views? Based on product reviews, can we predict the star rating of a product?\n",
        "\n",
        "<font color='blue'>*Translation*</font> - Recognize words in one language to provide similar words in another.\n",
        "\n",
        "**Playground:** https://www.deepl.com/translator\n",
        "\n",
        "\n",
        "<font color='blue'>*Summarization*</font> - Take a long document and produce a shorter one (a synthesis) without losing meaningful information.\n",
        "\n",
        "\n",
        "<font color='forestgreen'>**Methods:**</font> <span style=\"font-family:Calibri; color:red; font-size:12pt;\">The main idea is to quantify the occurrence of relevant words and, based on the context, to map them into vectors. That is to say that we want to create mathematically representable quantities from words and text; they will serve as features for data analysis. One approach is separate the text data into sentences and then sentences can be used to extract (key) words and expressions.</span>\n",
        "\n",
        "###**Regular Expressions (regex)**\n",
        "\n",
        "Goal: provide a language that allows us to search for different text strings.\n",
        "\n",
        "For example, Regular Expressions (frequently called “regex”) allows us to label all tweets with a “1” if they contain the following list of words:\n",
        "\n",
        "- college\n",
        "- College of\n",
        "- colleges\n",
        "- The College\n",
        "\n",
        "The idea is to detect that in all expressions above we have the same concept \"college\".\n",
        "\n",
        "\n",
        "\n",
        "<font color='blue' face='Calibri' size=5pt>Examples of common REGEX patterns</font>\n",
        "\n",
        "**[tT]**imber  - would match lower or uppercase T\n",
        "\n",
        "**[A-Z]** - would match any capital character\n",
        "\n",
        "**[a-z]** - would match any lowercase character\n",
        "\n",
        "**[0-9]** - would match any single number (i.e., 9)\n",
        "\n",
        "**[^A-Z]** - would match anything that isn’t an uppercase letter.\n",
        "\n",
        "**\\w** - would match any letter.\n",
        "\n",
        "A comprehensive manual on regex can be found here:\n",
        "https://www3.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjXxyULYjYT9"
      },
      "source": [
        "# import regex in Python\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XnTsc2O60Ol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1931590e-f5cd-42ad-9c74-f073d1f877b7"
      },
      "source": [
        "pattern = \"[cC]hoca\"\n",
        "sentence1 = \"Chocolate is very delicious\"\n",
        "sentence2 = \"This new recipe deliciously implemented a new idea about the texture of the chocolate.\"\n",
        "if re.search(pattern, sentence1):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not a match!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c7uzyAEoroFB",
        "outputId": "83e4ea9f-60b1-4312-aa1f-735bd15ec0b5"
      },
      "source": [
        "pattern"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[cC]hocolate'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BaUHIou70ez"
      },
      "source": [
        "###An example for replacing the spaces between words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKtrHwsD65vM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "def664c6-e099-4b5b-d9c4-a235c9ae1ebe"
      },
      "source": [
        "text = \"This chocolate is delicious but it may have too many calories, such as 400.\"\n",
        "re.sub('[^a-zA-Z0-9-,]','*',text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This*chocolate*is*delicious*but*it*may*have*too*many*calories,*such*as*400*'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnJyZUQ5D_Uj"
      },
      "source": [
        "text = \"This chocolate is delicious but it may have too many calories, such as 400.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8alHKVvSKAZ",
        "outputId": "69ec6b49-f9ff-471b-e2f8-28ec434536c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'chocolate',\n",
              " 'is',\n",
              " 'delicious',\n",
              " 'but',\n",
              " 'it',\n",
              " 'may',\n",
              " 'have',\n",
              " 'too',\n",
              " 'many',\n",
              " 'calories,',\n",
              " 'such',\n",
              " 'as',\n",
              " '400.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYZnVLP7D_bo"
      },
      "source": [
        "info = text.split(sep=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J3FhdSloEXyl",
        "outputId": "aa1c0b5f-c418-44cf-e1ba-7d4ec77dacec"
      },
      "source": [
        "# info is now an array of different words\n",
        "info[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'is'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dk4zqK08L7y"
      },
      "source": [
        "###An example for matching a patttern (a sequence of characters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL91n3Zy7x08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d534673-0772-4799-851e-7f046c9405d7"
      },
      "source": [
        "pattern = r\"[cC]hoco\"\n",
        "sequence = \"Chocolate is delicious\"\n",
        "if re.match(pattern, sequence):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxp0kAGuMc2y"
      },
      "source": [
        "### Rooting words is very important ! (in short, an identifier of the meaning of the word)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAp3N5p88Wwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16feeed9-835d-4f08-d621-efc73115b9f2"
      },
      "source": [
        "pattern = r\"good for you\"\n",
        "sentence = \"Chocolate is delicious and good for you\"\n",
        "if re.search(pattern, sentence):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Match!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeXHM2fn9zil"
      },
      "source": [
        "###Example:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1AMHbSgq3MHcv8Q8ljnHvl5IkxTKkzGkx' \n",
        "width='600px' />\n",
        "<figcaption>Data from Twitter</figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr1AOY-18eRD"
      },
      "source": [
        "text = \"\"\"Rep. Stephanie Murphy Verified account @RepStephMurphy Aug 30 More Celebrating 100yrs of coeducation at @williamandmary, \n",
        "        it was a true honor to return to my alma mater & join its first female president, Katherine Rowe, to welcome students at their convocation. \n",
        "        I spoke about the power of patriotism & the urgent need for active, engaged citizens.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51YBSini_hUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f71f7c-9193-4573-c9ff-5089a1b0a8af"
      },
      "source": [
        "pattern = r\"[cC]elebrating\"\n",
        "if re.search(pattern, text):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfxPZBsd_mXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3014188-5867-4763-c4de-8f3d8a6d56ed"
      },
      "source": [
        "pattern = r\"\\welebrat[a-z]+\"\n",
        "if re.search(pattern, text):\n",
        "  print(\"Match!\")\n",
        "else: print(\"Not a match!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Match!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WE1A7C8AeVy"
      },
      "source": [
        "<font face='Calibri' color='blue' size=5pt>The Bag of Words model (BoW)</font>\n",
        "\n",
        "**Main Goal:** use concurrences within context and counts of keywords to make predictions.\n",
        "\n",
        "**Observation:** there are many words that do not matter (such as prepositions or definite and indefinite articles). \n",
        "\n",
        "**Important:** each word can be translated into a binary value of occurrence.\n",
        "\n",
        "<span style=\"font-family:Calibri; color:darkblue; font-size:5pt;\">Analog Example:</span>\n",
        "\n",
        "*Statement 1*: Jurassic World was the pinnacle of human achievement.\n",
        "\n",
        "*Statement 2*: Human kind would be better without Jurassic World.\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1EUGNgop58BOOhFGHR3iKs5gXbrji6jEM' \n",
        "width='600px' />\n",
        "<figcaption>What is the difference in the statements above?</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "**Method**: we discard the *stopwords* such as articles, prepositions, verbs and retain the *corpus* (important words or *roots* of important words).\n",
        "\n",
        "\n",
        "\n",
        "A simple model based on this data:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1-uuXfXiYlmub8DauhxhYYCP2TKqfdvoB' \n",
        "width='600px' />\n",
        "<figcaption>The differences can be highlighted by using a count/vectorizer method</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "**Main idea:** analyze differences and co-occurrencies.\n",
        "\n",
        "**Known Problems:**\n",
        "\n",
        " - If some sentences are much longer in length, the vocabulary would increase and as such, the length of the vectors would increase; this is a dimensionality problem.\n",
        " - The new sentences may contain more different words from the previous sentences.\n",
        " - The vectors would also contain many zeros, thereby resulting in a sparse matrix.\n",
        " - No information on the grammatical structure or the actual ordering of the words is being used.\n",
        "\n",
        "**Possible Solution:** Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "The term frequency-inverse document frequency is a measure that quantifies the importance of a word in the context of a document or a *corpus*.\n",
        "\n",
        "The *term-frequency* of a word is the relative frequency of the term in the context of the document.\n",
        "\n",
        "$$\\text{TF}(t,d):=\\frac{\\text{# of times the term appears in the document}}{\\text{# of terms in the document }}$$\n",
        "\n",
        "\n",
        "The *inverse document frequency* is defined as:\n",
        "\n",
        "$$\\text{IDF}(t,d):=\\log\\left(\\frac{\\text{# of documents}}{\\text{# of documents with term } t}\\right)$$\n",
        "\n",
        "Our quantification of relative importance is defined as the product between TF and IDF.\n",
        "\n",
        "TF-IDF gives larger values for less frequent words and is high when both IDF and TF values are high, for instance the word is rare in all the documents combined but frequent in a single document.\n",
        "\n",
        "A good Python example can be found here: \n",
        "\n",
        "https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-rVM7uOAFj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0765e41-9c91-4765-ccb3-adca9107702e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcgT03h3BW-t"
      },
      "source": [
        "text = open(\"drive/MyDrive/Data Sets/SherlockHolmes.txt\").read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmmUvIi4YTSy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b63578f1-674b-46fb-905e-4578903df5ac"
      },
      "source": [
        "text[:1234]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg eBook of The Adventures of Sherlock Holmes, by Arthur Conan Doyle\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org. If you are not located in the United States, you\\nwill have to check the laws of the country where you are located before\\nusing this eBook.\\n\\nTitle: The Adventures of Sherlock Holmes\\n\\nAuthor: Arthur Conan Doyle\\n\\nRelease Date: November 29, 2002 [eBook #1661]\\n[Most recently updated: May 20, 2019]\\n\\nLanguage: English\\n\\nCharacter set encoding: UTF-8\\n\\nProduced by: an anonymous Project Gutenberg volunteer and Jose Menendez\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES ***\\n\\ncover\\n\\n\\n\\n\\nThe Adventures of Sherlock Holmes\\n\\nby Arthur Conan Doyle\\n\\n\\nContents\\n\\n   I.     A Scandal in Bohemia\\n   II.    The Red-Headed League\\n   III.   A Case of Identity\\n   IV.    The Boscombe Valley Mystery\\n   V.     The Five Orange Pips\\n   VI.    The Man with the Twisted Lip\\n   VII.   The Adventure of the Blue Carbuncle\\n   VIII.  The Adv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwPgD4RsBeVS"
      },
      "source": [
        "<font face=\"Calibri\" color='navy' size=4pt>We can extract all the sentences (based on punctuation):</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PobVsJyzB9mJ"
      },
      "source": [
        "dataset = nltk.sent_tokenize(text) \n",
        "for i in range(len(dataset)): \n",
        "    dataset[i] = dataset[i].lower() \n",
        "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
        "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "InfAZVegJWFY",
        "outputId": "b1749632-eeaf-45e6-fd20-9781f8e122d5"
      },
      "source": [
        "dataset[1000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'but the maiden herself was most instructive you appeared to read a good deal upon her which was quite invisible to me i remarked '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIAkuuEdCEZ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7c4f3043-be8e-4f9a-f8f9-0953cecc08f8"
      },
      "source": [
        "# this is the 2001th sentence\n",
        "dataset[2000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'they could only have come from the old man at my side and yet he sat now as absorbed as ever very thin very wrinkled bent with age an opium pipe dangling down from between his knees as though it had dropped in sheer lassitude from his fingers '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VbuMt_-3gNt"
      },
      "source": [
        "What do you notice? There are no capital letters, no punctuation (because the computer does not need them).\n",
        "\n",
        "We can also determine how frequent are the different words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liv9PCWGCOp7"
      },
      "source": [
        "# we can count the occurrencies of different words\n",
        "# Creating the Bag of Words model \n",
        "word2count = {} # this is a list \n",
        "for data in dataset: \n",
        "    words = nltk.word_tokenize(data) \n",
        "    for word in words: \n",
        "        if word not in word2count.keys(): \n",
        "            word2count[word] = 1\n",
        "        else: \n",
        "            word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdglAkPpCZp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bac1bb7-a216-4937-eee9-ffb45bb441dc"
      },
      "source": [
        "word2count.get('follow')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7AscS7Z2FnJ",
        "outputId": "448603c3-5160-4eb2-eb17-9c4f929db3d9"
      },
      "source": [
        "word2count.get('watson')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgPnOqmDOzp"
      },
      "source": [
        "This means that the word \"ghost\" appeared 1 time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icqKTzLnCaZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3bfed2-e97a-497e-9301-8c963ce91ce1"
      },
      "source": [
        "word2count.get('the') # however 'the' is a stopword so it should be counted!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5815"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81WOQYhDQn5"
      },
      "source": [
        "<font face=\"Calibri\" color='navy' size=4pt>We can determine what are the most frequent words, for example:</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT3CP1V4DhxL"
      },
      "source": [
        "# the top 100 most frequent words\n",
        "import heapq \n",
        "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
        "freq_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldxUtPD_EIZs"
      },
      "source": [
        "Indeed this is a story about \"sherlock\" and \"crime\"...\n",
        "\n",
        "Important: we want to discard all the unimportant words (as known as \"stopwords\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnDYV22LEAKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dce66b7-af64-4077-c54c-c86b139649d0"
      },
      "source": [
        "# Stopword dictionary\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# For stemming\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cv4uf8jEmfY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dea5fbc3-5e3d-4b4d-d749-986d2ec268f6"
      },
      "source": [
        "txt = re.sub('[^a-zA-Z0-9 ]','',dataset[2300])\n",
        "# Make everything lower case\n",
        "txt = txt.lower()\n",
        "# Make it a list of words\n",
        "txt = txt.split()\n",
        "# Get all the stop words out\n",
        "txt = [word for word in txt if not word in set(stopwords.words('english'))]\n",
        "# Stem the words\n",
        "txt = [stemmer.stem(word) for word in txt]\n",
        "# Put it all back together and look at the result\n",
        "' '.join(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'day stream penni vari silver pour upon bad day fail take 2'"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cShN8rfsEyL5"
      },
      "source": [
        "..and we want to do this for every sentence in the book:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZkkVIhVExLH"
      },
      "source": [
        "corpus = [] # the name 'corpus' refers to the senteces after we throwed all stopwords and we rooted the remaining ones\n",
        "for i in range(len(dataset)):\n",
        "    txt = re.sub('[^a-zA-Z0-9 ]','',dataset[i])\n",
        "    txt = txt.lower()\n",
        "    txt = txt.split()\n",
        "    txt = [word for word in txt if not word in set(stopwords.words('english'))]\n",
        "    txt = [stemmer.stem(word) for word in txt]\n",
        "    txt = ' '.join(txt)\n",
        "    corpus.append(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n2VU3JwE4DL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06ccf91-396b-446e-ff44-e3cbbc0f28e6"
      },
      "source": [
        "corpus[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['project gutenberg ebook adventur sherlock holm arthur conan doyl ebook use anyon anywher unit state part world cost almost restrict whatsoev',\n",
              " 'may copi give away use term project gutenberg licens includ ebook onlin www gutenberg org',\n",
              " 'locat unit state check law countri locat use ebook',\n",
              " 'titl adventur sherlock holm author arthur conan doyl releas date novemb 29 2002 ebook 1661 recent updat may 20 2019 languag english charact set encod utf 8 produc anonym project gutenberg volunt jose menendez start project gutenberg ebook adventur sherlock holm cover adventur sherlock holm arthur conan doyl content',\n",
              " 'scandal bohemia ii',\n",
              " 'red head leagu iii',\n",
              " 'case ident iv',\n",
              " 'boscomb valley mysteri v five orang pip vi',\n",
              " 'man twist lip vii',\n",
              " 'adventur blue carbuncl viii',\n",
              " 'adventur speckl band ix',\n",
              " 'adventur engin thumb x',\n",
              " 'adventur nobl bachelor xi',\n",
              " 'adventur beryl coronet xii',\n",
              " 'adventur copper beech',\n",
              " 'scandal bohemia',\n",
              " 'sherlock holm alway woman',\n",
              " 'seldom heard mention name',\n",
              " 'eye eclips predomin whole sex',\n",
              " 'felt emot akin love iren adler',\n",
              " 'emot one particularli abhorr cold precis admir balanc mind',\n",
              " 'take perfect reason observ machin world seen lover would place fals posit',\n",
              " 'never spoke softer passion save gibe sneer',\n",
              " 'admir thing observ excel draw veil men motiv action',\n",
              " 'train reason admit intrus delic fine adjust tempera introduc distract factor might throw doubt upon mental result',\n",
              " 'grit sensit instrument crack one high power lens would disturb strong emot natur',\n",
              " 'yet one woman woman late iren adler dubiou question memori',\n",
              " 'seen littl holm late',\n",
              " 'marriag drift us away',\n",
              " 'complet happi home centr interest rise around man first find master establish suffici absorb attent holm loath everi form societi whole bohemian soul remain lodg baker street buri among old book altern week week cocain ambit drowsi drug fierc energi keen natur',\n",
              " 'still ever deepli attract studi crime occupi immens faculti extraordinari power observ follow clue clear mysteri abandon hopeless offici polic',\n",
              " 'time time heard vagu account do summon odessa case trepoff murder clear singular tragedi atkinson brother trincomale final mission accomplish delic success reign famili holland',\n",
              " 'beyond sign activ howev mere share reader daili press knew littl former friend companion',\n",
              " 'one night twentieth march 1888 return journey patient return civil practic way led baker street',\n",
              " 'pass well rememb door must alway associ mind woo dark incid studi scarlet seiz keen desir see holm know employ extraordinari power',\n",
              " 'room brilliantli lit even look saw tall spare figur pass twice dark silhouett blind',\n",
              " 'pace room swiftli eagerli head sunk upon chest hand clasp behind',\n",
              " 'knew everi mood habit attitud manner told stori',\n",
              " 'work',\n",
              " 'risen drug creat dream hot upon scent new problem',\n",
              " 'rang bell shown chamber formerli part',\n",
              " 'manner effus',\n",
              " 'seldom glad think see',\n",
              " 'hardli word spoken kindli eye wave armchair threw across case cigar indic spirit case gasogen corner',\n",
              " 'stood fire look singular introspect fashion',\n",
              " 'wedlock suit remark',\n",
              " 'think watson put seven half pound sinc saw seven answer',\n",
              " 'inde thought littl',\n",
              " 'trifl fanci watson',\n",
              " 'practic observ',\n",
              " 'tell intend go har know see deduc',\n",
              " 'know get wet late clumsi careless servant girl dear holm said much',\n",
              " 'would certainli burn live centuri ago',\n",
              " 'true countri walk thursday came home dread mess chang cloth imagin deduc',\n",
              " 'mari jane incorrig wife given notic fail see work chuckl rub long nervou hand togeth',\n",
              " 'simplic said eye tell insid left shoe firelight strike leather score six almost parallel cut',\n",
              " 'obvious caus someon carelessli scrape round edg sole order remov crust mud',\n",
              " 'henc see doubl deduct vile weather particularli malign boot slit specimen london slavey',\n",
              " 'practic gentleman walk room smell iodoform black mark nitrat silver upon right forefing bulg right side top hat show secret stethoscop must dull inde pronounc activ member medic profess could help laugh eas explain process deduct',\n",
              " 'hear give reason remark thing alway appear ridicul simpl could easili though success instanc reason baffl explain process',\n",
              " 'yet believ eye good quit answer light cigarett throw armchair',\n",
              " 'see observ',\n",
              " 'distinct clear',\n",
              " 'exampl frequent seen step lead hall room frequent often well hundr time mani mani',\n",
              " 'know quit',\n",
              " 'observ',\n",
              " 'yet seen',\n",
              " 'point',\n",
              " 'know seventeen step seen observ',\n",
              " 'way sinc interest littl problem sinc good enough chronicl one two trifl experi may interest threw sheet thick pink tint notepap lie open upon tabl',\n",
              " 'came last post said',\n",
              " 'read aloud note undat without either signatur address',\n",
              " 'call upon night quarter eight clock said gentleman desir consult upon matter deepest moment',\n",
              " 'recent servic one royal hous europ shown one may safe trust matter import hardli exagger',\n",
              " 'account quarter receiv',\n",
              " 'chamber hour take amiss visitor wear mask inde mysteri remark',\n",
              " 'imagin mean data yet',\n",
              " 'capit mistak theoris one data',\n",
              " 'insens one begin twist fact suit theori instead theori suit fact',\n",
              " 'note',\n",
              " 'deduc care examin write paper upon written',\n",
              " 'man wrote presum well remark endeavour imit companion process',\n",
              " 'paper could bought half crown packet',\n",
              " 'peculiarli strong stiff peculiar word said holm',\n",
              " 'english paper',\n",
              " 'hold light saw larg e small g p larg g small woven textur paper',\n",
              " 'make ask holm',\n",
              " 'name maker doubt monogram rather',\n",
              " 'g small stand gesellschaft german compani customari contract like co p cours stand papier eg let us glanc continent gazett took heavi brown volum shelv',\n",
              " 'eglow eglonitz egria',\n",
              " 'german speak countri bohemia far carlsbad',\n",
              " 'remark scene death wallenstein numer glass factori paper mill ha ha boy make eye sparkl sent great blue triumphant cloud cigarett',\n",
              " 'paper made bohemia said',\n",
              " 'precis',\n",
              " 'man wrote note german',\n",
              " 'note peculiar construct sentenc account quarter receiv frenchman russian could written',\n",
              " 'german uncourt verb',\n",
              " 'remain therefor discov want german write upon bohemian paper prefer wear mask show face',\n",
              " 'come mistaken resolv doubt spoke sharp sound hors hoof grate wheel curb follow sharp pull bell',\n",
              " 'holm whistl']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZOJeHKFA7r"
      },
      "source": [
        "# we can count the occurrencies of different words in the corpus\n",
        "# Creating the Bag of Words model \n",
        "word2count = {} \n",
        "for data in corpus: \n",
        "    words = nltk.word_tokenize(data) \n",
        "    for word in words: \n",
        "        if word not in word2count.keys(): \n",
        "            word2count[word] = 1\n",
        "        else: \n",
        "            word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbheF3ELFGyq"
      },
      "source": [
        "# .. and get the top 10 most frequent in the corpus:\n",
        "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
        "freq_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NGVIDzFL4J"
      },
      "source": [
        "## Application to Amazon customer reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeNYrf-JcNp_"
      },
      "source": [
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soXF4QCQFOtz"
      },
      "source": [
        "# this data is available via Kaggle\n",
        "df = pd.read_csv('drive/MyDrive/Data Sets/amazon_reviews.csv', quoting=2 )\n",
        "# Extract the ratings and text reviews\n",
        "data = df[['reviews.text', 'reviews.rating']].dropna().reset_index(drop=True)\n",
        "\n",
        "reviews = data['reviews.text']\n",
        "y = data['reviews.rating']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n92B5NCDcSRI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dde37bf4-6a86-4235-a93a-e3c51e256c09"
      },
      "source": [
        "data.loc[500,'reviews.text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I have a regular echo and now the tap. Both awesome products, use them to control lights, locks, and play music. Would buy again.'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeIav12x5CbA",
        "outputId": "4307f422-fc36-49b3-d192-1948ed30d182"
      },
      "source": [
        "y[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDhklObNFb-V"
      },
      "source": [
        "To learn more about the data:   \n",
        "\n",
        "https://www.kaggle.com/bittlingmayer/amazonreviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_7DersbFcfR"
      },
      "source": [
        "allreviews = []\n",
        "for i in range(len(reviews)):\n",
        "    txt = re.sub('[^a-zA-Z0-9 ]','',reviews[i])\n",
        "    txt = txt.lower()\n",
        "    txt = txt.split()\n",
        "    txt = [word for word in txt if not word in set(stopwords.words('english'))]\n",
        "    txt = [stemmer.stem(word) for word in txt]\n",
        "    txt = ' '.join(txt)\n",
        "    allreviews.append(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRgcjbYWIRRP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1045882-a7c6-4a95-bdcd-6d43e908b388"
      },
      "source": [
        "allreviews[500] # this btw is a 5 star review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'regular echo tap awesom product use control light lock play music would buy'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWb4ZixhFpQ1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X_raw = cv.fit_transform(allreviews)\n",
        "X = X_raw.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUByyjezXNxZ",
        "outputId": "eb4ffa17-c0ce-4426-db10-936074ced43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1177, 5084)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu0wfsfugkdS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXibZEqWdPPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cdce01d-fc0a-4889-b2cc-fb7fa928d373"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1177, 5084)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukfkEpSMFuie"
      },
      "source": [
        "# for the number of stars we say 5 star is a hit and less than 5 is a miss\n",
        "yb = y.where(y==5, other=0).where(y<5, other=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD3-Corc6O1E",
        "outputId": "71970ca7-29f4-4d7d-d932-ba2f4d15fba7"
      },
      "source": [
        "yb[309]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aig3TTew6SKJ",
        "outputId": "9392b65f-6044-4022-f726-19c3ea322bad"
      },
      "source": [
        "yb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1177,)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqSbY30WIx1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1059b6-c049-4dec-b372-1401cec139d6"
      },
      "source": [
        "yb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       1.0\n",
              "1       1.0\n",
              "2       0.0\n",
              "3       1.0\n",
              "4       1.0\n",
              "       ... \n",
              "1172    0.0\n",
              "1173    0.0\n",
              "1174    0.0\n",
              "1175    0.0\n",
              "1176    0.0\n",
              "Name: reviews.rating, Length: 1177, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Do_3AbH9IG"
      },
      "source": [
        "### Logistic Regression Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSBtBGAbF9Uj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "9cac4ac6-ce5c-41da-e264-78a73a855edf"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "\n",
        "Xtrain,Xtest,ytrain,ytest = tts(X,yb,random_state=310,test_size=0.25)\n",
        "cls = LogisticRegression(random_state=310, solver='lbfgs')\n",
        "cls.fit(Xtrain,ytrain)\n",
        "ypred = cls.predict(Xtest)\n",
        "cm = confusion_matrix(ytest, ypred)\n",
        "pd.DataFrame(cm, columns=['Not 5', '5'], index =['Not 5', '5'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Not 5</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Not 5</th>\n",
              "      <td>59</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Not 5    5\n",
              "Not 5     59   57\n",
              "5         30  149"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIO4S8lWJjdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7907a2-c02f-4500-8dc4-3a6695c92955"
      },
      "source": [
        "acc(ytest,ypred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7050847457627119"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z-wgGbY6wVR",
        "outputId": "eded576a-4597-430c-a1c8-74be076ac03d"
      },
      "source": [
        "# the input features are based on the Bag of Words Model\n",
        "# the input features matrix X is sparse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors"
      ],
      "metadata": {
        "id": "YOFJYAd-mNP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "alkt9kTzmFqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KNeighborsClassifier(n_neighbors=5,weights='distance')"
      ],
      "metadata": {
        "id": "7yiHGREFmRQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(Xtrain,ytrain)\n",
        "ypred = model.predict(Xtest)\n",
        "cm = confusion_matrix(ytest, ypred)\n",
        "pd.DataFrame(cm, columns=['Not 5', '5'], index =['Not 5', '5'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "Pnm2EprTmRWF",
        "outputId": "2184a2cf-23a0-448b-9e62-44a8f562dccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Not 5</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Not 5</th>\n",
              "      <td>43</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21</td>\n",
              "      <td>158</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Not 5    5\n",
              "Not 5     43   73\n",
              "5         21  158"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNNCmCZwH_ky"
      },
      "source": [
        "### Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR17RiZMHkHa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "83de23c0-35f7-4810-cb03-92bb25af50f1"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "cls = GaussianNB()\n",
        "cls.fit(Xtrain,ytrain)\n",
        "ypred = cls.predict(Xtest)\n",
        "cm = confusion_matrix(ytest, ypred)\n",
        "pd.DataFrame(cm, columns=['Not 5', '5'], index =['Not 5', '5'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Not 5</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Not 5</th>\n",
              "      <td>82</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>71</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Not 5    5\n",
              "Not 5     82   34\n",
              "5         71  108"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy4_0A11eZ6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a4f843-b82e-49aa-981a-9755802ca3cd"
      },
      "source": [
        "acc(ytest,ypred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6440677966101694"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pea2T1NnIEC7"
      },
      "source": [
        "### Random Forest Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9dpA18ZF6HZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "f4c4cd9d-b6bf-4fdb-e805-ef6ba254c6d5"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "cls = RandomForestClassifier(random_state=310, max_depth=100, n_estimators = 100)\n",
        "cls.fit(Xtrain,ytrain)\n",
        "ypred = cls.predict(Xtest)\n",
        "cm = confusion_matrix(ytest, ypred)\n",
        "pd.DataFrame(cm, columns=['Not 5', '5'], index =['Not 5', '5'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Not 5</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Not 5</th>\n",
              "      <td>47</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Not 5    5\n",
              "Not 5     47   69\n",
              "5         12  167"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va1QXBwpetaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29d4c17-0020-49b1-a452-66e7545654bd"
      },
      "source": [
        "acc(ytest,ypred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7254237288135593"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jGLmCiMI_sg"
      },
      "source": [
        "## Application to wine ratings based on customer reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUtoy0hQI_Io"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk import download\n",
        "download('stopwords')\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GckK-WM1JlgQ"
      },
      "source": [
        "%%time\n",
        "wine_data = pd.read_csv('winemagdata130kv2.csv',quoting=2)\n",
        "wines = wine_data[[\"description\",\"points\"]]\n",
        "wines_subset = wines.sample(1000,random_state=1693).reset_index(drop=True)\n",
        "corpus = []\n",
        "\n",
        "for i in range(0,len(wines_subset)):\n",
        "    wine_descriptions = re.sub('[^a-zA-Z0-9 ]','',wines_subset[\"description\"][i])\n",
        "    wine_descriptions=wine_descriptions.lower()\n",
        "    wine_descriptions = wine_descriptions.split()\n",
        "    wine_descriptions = [word for word in wine_descriptions if not word in set(stopwords.words('english'))]\n",
        "    stemmer = PorterStemmer()\n",
        "    wine_descriptions = [stemmer.stem(word) for word in wine_descriptions]\n",
        "    wine_descriptions = \" \".join(wine_descriptions)\n",
        "    corpus.append(wine_descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSlRbCJpJ2uO"
      },
      "source": [
        "%%time\n",
        "countVec = CountVectorizer()\n",
        "X_raw = countVec.fit_transform(corpus)\n",
        "X = X_raw.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-0xqoC4J_X0"
      },
      "source": [
        "#### Visualize the distribution of the wine ratings (points)\n",
        "n, bins, patches = plt.hist(wines_subset[\"points\"].values,10,density=1,facecolor='green',alpha=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ASsfjQKCro"
      },
      "source": [
        "y = wines_subset[\"points\"]\n",
        "y = y.where(y>90,other=0).where(y<=90,other=1).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlkycIzvKID2"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = tts(X,y,test_size=0.25,random_state=1693)\n",
        "#scale_X = StandardScaler()\n",
        "#X_train = scale_X.fit_transform(X_train)\n",
        "#X_test = scale_X.transform(X_test)\n",
        "classifier = LogisticRegression(random_state=1693,solver='lbfgs')\n",
        "classifier.fit(X_train,Y_train)\n",
        "Y_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3FQUwCtKLdV"
      },
      "source": [
        "spc = ['Bad','Good']\n",
        "cm = confusion_matrix(Y_test,Y_pred)\n",
        "pd.DataFrame(cm, columns=spc, index=spc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}