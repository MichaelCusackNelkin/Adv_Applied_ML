{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import lstsq\n",
    "from scipy.sparse.linalg import lsmr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d, griddata, LinearNDInterpolator, NearestNDInterpolator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split as tts\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# import libraries for creating a neural network\n",
    "# imports for creating a Neural Network\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop # they recently updated Tensorflow\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#XGB Boosting\n",
    "import xgboost as xgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tricubic Kernel\n",
    "def Tricubic(x):\n",
    "  if len(x.shape) == 1:\n",
    "    x = x.reshape(-1,1)\n",
    "  d = np.sqrt(np.sum(x**2,axis=1))\n",
    "  return np.where(d>1,0,70/81*(1-d**3)**3)\n",
    "\n",
    "# Quartic Kernel\n",
    "def Quartic(x):\n",
    "  if len(x.shape) == 1:\n",
    "    x = x.reshape(-1,1)\n",
    "  d = np.sqrt(np.sum(x**2,axis=1))\n",
    "  return np.where(d>1,0,15/16*(1-d**2)**2)\n",
    "\n",
    "# Epanechnikov Kernel\n",
    "def Epanechnikov(x):\n",
    "  if len(x.shape) == 1:\n",
    "    x = x.reshape(-1,1)\n",
    "  d = np.sqrt(np.sum(x**2,axis=1))\n",
    "  return np.where(d>1,0,3/4*(1-d**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the kernel local regression model\n",
    "\n",
    "def lw_reg(X, y, xnew, kern, tau, intercept):\n",
    "    # tau is called bandwidth K((x-x[i])/(2*tau))\n",
    "    n = len(X) # the number of observations\n",
    "    yest = np.zeros(n)\n",
    "\n",
    "    if len(y.shape)==1: # here we make column vectors\n",
    "      y = y.reshape(-1,1)\n",
    "\n",
    "    if len(X.shape)==1:\n",
    "      X = X.reshape(-1,1)\n",
    "    \n",
    "    if intercept:\n",
    "      X1 = np.column_stack([np.ones((len(X),1)),X])\n",
    "    else:\n",
    "      X1 = X\n",
    "\n",
    "    w = np.array([kern((X - X[i])/(2*tau)) for i in range(n)]) # here we compute n vectors of weights\n",
    "\n",
    "    #Looping through all X-points\n",
    "    for i in range(n):          \n",
    "        W = np.diag(w[:,i])\n",
    "        b = np.transpose(X1).dot(W).dot(y)\n",
    "        A = np.transpose(X1).dot(W).dot(X1)\n",
    "        #A = A + 0.001*np.eye(X1.shape[1]) # if we want L2 regularization\n",
    "        #theta = linalg.solve(A, b) # A*theta = b\n",
    "        beta, res, rnk, s = lstsq(A, b)\n",
    "        yest[i] = np.dot(X1[i],beta)\n",
    "    if X.shape[1]==1:\n",
    "      f = interp1d(X.flatten(),yest,fill_value='extrapolate')\n",
    "    else:\n",
    "      f = LinearNDInterpolator(X, yest)\n",
    "    output = f(xnew) # the output may have NaN's where the data points from xnew are outside the convex hull of X\n",
    "    if sum(np.isnan(output))>0:\n",
    "      g = NearestNDInterpolator(X,y.ravel()) \n",
    "      # output[np.isnan(output)] = g(X[np.isnan(output)])\n",
    "      output[np.isnan(output)] = g(xnew[np.isnan(output)])\n",
    "    return output\n",
    "\n",
    "#Boosted LWR with decision tree or random forest boosting\n",
    "def boosted_lwr(X, y, xnew, kern, tau, intercept): \n",
    "  # we need decision trees\n",
    "  # for training the boosted method we use X and y\n",
    "  Fx = lw_reg(X,y,X,kern,tau,intercept) # we need this for training the Decision Tree\n",
    "  # Now train the Decision Tree on y_i - F(x_i)\n",
    "  new_y = y - Fx\n",
    "  #model = DecisionTreeRegressor(max_depth=2, random_state=123)\n",
    "  model = RandomForestRegressor(n_estimators=100,max_depth=2)\n",
    "  #model = model_xgb\n",
    "  model.fit(X,new_y)\n",
    "  output = model.predict(xnew) + lw_reg(X,y,xnew,kern,tau,intercept)\n",
    "  return output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we design a Nerual Network for regression\n",
    "\n",
    "How many layers? How many neurons in each layer? All dense layers (MLP)? What other types? Which activation functions do we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('Data/cars.csv')\n",
    "\n",
    "X = cars[['ENG','CYL','WGT']].values\n",
    "y = cars['MPG'].values\n",
    "\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
    "scale = StandardScaler()\n",
    "\n",
    "data = np.concatenate([X,y.reshape(-1,1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = Sequential() #Making a tensorflow sequential network\n",
    "\n",
    "#Arbitrary number of layers, try 5? Lets do 128-neuron dense layers with relu activation layers\n",
    "model_nn.add(Dense(128, activation=\"relu\", input_dim=3))\n",
    "model_nn.add(Dropout(0.1))\n",
    "model_nn.add(Dense(64, activation=\"relu\"))\n",
    "model_nn.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "#Doing a linear regression, so linear activation in the last layer (and only one output dim 1)\n",
    "#It's solving a regression problem, so it's predicting a continuous random variable, meaning the activation of the last\n",
    "#neuron is the continuous random variable to be predicted.\n",
    "model_nn.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "#Optimizer is doing gradient descent on the weights and biases and updating them via backpropogation\n",
    "model_nn.compile(loss='mean_squared_error', optimizer=Adam(learning_rate= 0.001, decay = 0.0001))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=600) #Stops the training loop if it's taking too long(gets stuck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The XGB model definition\n",
    "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold Validation Number: 1\n",
      "\n",
      "Split Number: 1\n",
      "Split Number: 2\n",
      "Split Number: 3\n",
      "Split Number: 4\n",
      "Split Number: 5\n",
      "Split Number: 6\n",
      "Split Number: 7\n",
      "Split Number: 8\n",
      "Split Number: 9\n",
      "Split Number: 10\n",
      "\n",
      " The Cross-validated Mean Squared Error for LWR is : 16.901595996974294\n",
      "The Cross-validated Mean Squared Error for BLWR is : 16.664261054158153\n",
      "The Cross-validated Mean Squared Error for RF is : 16.919413487963148\n",
      "The Cross-validated Mean Squared Error for XGB is : 16.25007368020466\n",
      "The Cross-validated Mean Squared Error for NN is : 20.88730931540277\n"
     ]
    }
   ],
   "source": [
    "# we want more nested cross-validations\n",
    "#This block will take an hour of more (long time)\n",
    "#Neural networks are usually no match for boosted algorithms when it comes to regression on a continuos variable\n",
    "\n",
    "mse_lwr = []\n",
    "mse_blwr = []\n",
    "mse_rf = []\n",
    "mse_xgb = []\n",
    "mse_nn = []\n",
    "\n",
    "for i in range(1):\n",
    "  print('KFold Validation Number: ' + str(i+1) + '\\n')\n",
    "  kf = KFold(n_splits=10,shuffle=True,random_state=i)\n",
    "  # this is the Cross-Validation Loop\n",
    "  j = 0\n",
    "  for idxtrain, idxtest in kf.split(X):\n",
    "    j += 1\n",
    "    #Split the train and test data\n",
    "    xtrain = X[idxtrain]\n",
    "    ytrain = y[idxtrain]\n",
    "    ytest = y[idxtest]\n",
    "    xtest = X[idxtest]\n",
    "    xtrain = scale.fit_transform(xtrain)\n",
    "    xtest = scale.transform(xtest)\n",
    "\n",
    "    #Train and predict for LWR and boosted LWR\n",
    "    yhat_lwr = lw_reg(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
    "    yhat_blwr = boosted_lwr(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
    "\n",
    "    #Train and predict with random forest\n",
    "    model_rf = RandomForestRegressor(n_estimators=100,max_depth=3)\n",
    "    model_rf.fit(xtrain,ytrain)\n",
    "    yhat_rf = model_rf.predict(xtest)\n",
    "\n",
    "    #Train and predict for XGB\n",
    "    model_xgb.fit(xtrain,ytrain)\n",
    "    yhat_xgb = model_xgb.predict(xtest)\n",
    "\n",
    "    #Train and predict for neural network\n",
    "    #Batch size devides the train data into batches, trains on each batch then updates the gradients until it gets through the whole data\n",
    "    #Then moves onto the next epoch\n",
    "    print('Split Number: ' + str(j))\n",
    "    model_nn.fit(xtrain,ytrain,validation_split=0.3, epochs=100, batch_size=20, verbose=0, callbacks=[es])\n",
    "    yhat_nn = model_nn.predict(xtest)\n",
    "\n",
    "    #Append each model's MSE\n",
    "    mse_lwr.append(mse(ytest,yhat_lwr))\n",
    "    mse_blwr.append(mse(ytest,yhat_blwr))\n",
    "    mse_rf.append(mse(ytest,yhat_rf))\n",
    "    mse_xgb.append(mse(ytest,yhat_xgb))\n",
    "    mse_nn.append(mse(ytest,yhat_nn))\n",
    "\n",
    "print('\\n The Cross-validated Mean Squared Error for LWR is : '+str(np.mean(mse_lwr)))\n",
    "print('The Cross-validated Mean Squared Error for BLWR is : '+str(np.mean(mse_blwr)))\n",
    "print('The Cross-validated Mean Squared Error for RF is : '+str(np.mean(mse_rf)))\n",
    "print('The Cross-validated Mean Squared Error for XGB is : '+str(np.mean(mse_xgb)))\n",
    "print('The Cross-validated Mean Squared Error for NN is : '+str(np.mean(mse_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next apply this to boston housing dataset and concrete strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feb 23rd -- Nadaraya-Watson Estimator\n",
    "\n",
    "Generally, we provide an estimation of Y based on X being a specific x, written $m(x) = E[Y | X=x]$\n",
    "\n",
    "For Nadaraya-Watson estimators, we use $m(x) = E[Y|X=x] = \\int y f_{Y|X=x}(y)dy = \\frac{\\int y f(x,y)dy}{f_{X}(x)}$\n",
    "\n",
    "So basically, the regression function m(x) can be estimated by joint probablity density function of X and Y, called $f$, and \"the marginal\" $f_x$\n",
    "\n",
    "This becomes:\n",
    "\n",
    "$\\hat f(x,y;h) = \\frac{1}{n} \\sum_{i=1}^n K_{h_1}(x-X_i) K_{h_2}(y-Y_i)$ where $K_{h_1}, K_{h_2}$ are the bandwiths in X and Y (like tau for loess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "model_KernReg = KernelReg(endog = data[:,-1], exog = data[:,:-1], var_type='ccc', ckertype='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.333316197606733\n"
     ]
    }
   ],
   "source": [
    "yhat, ystd = model_KernReg.fit(data[:,:-1]) #predictions\n",
    "mse_kern = mse(y,yhat)\n",
    "print(mse_kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold Validation Number: 1\n",
      "\n",
      "Split Number: 1\n",
      "Split Number: 2\n",
      "\n",
      "\n",
      "The Cross-validated Mean Squared Error for LWR is : 18.424762629746425\n",
      "The Cross-validated Mean Squared Error for BLWR is : 18.94945168954672\n",
      "The Cross-validated Mean Squared Error for RF is : 17.885075990581313\n",
      "The Cross-validated Mean Squared Error for XGB is : 17.88009879064699\n",
      "The Cross-validated Mean Squared Error for NN is : 20.36499785951939\n",
      "The Cross-validated Mean Squared Error for KernReg is : 17.73572469703669\n"
     ]
    }
   ],
   "source": [
    "# we want more nested cross-validations\n",
    "#This block will take an hour of more (long time)\n",
    "#Neural networks are usually no match for boosted algorithms when it comes to regression on a continuos variable\n",
    "\n",
    "mse_lwr = []\n",
    "mse_blwr = []\n",
    "mse_rf = []\n",
    "mse_xgb = []\n",
    "mse_nn = []\n",
    "mse_kern = []\n",
    "\n",
    "for i in range(1):\n",
    "  print('KFold Validation Number: ' + str(i+1) + '\\n')\n",
    "  kf = KFold(n_splits=2,shuffle=True,random_state=i)\n",
    "  # this is the Cross-Validation Loop\n",
    "  j = 0\n",
    "  for idxtrain, idxtest in kf.split(X):\n",
    "    j += 1\n",
    "    #Split the train and test data\n",
    "    xtrain = X[idxtrain]\n",
    "    ytrain = y[idxtrain]\n",
    "    ytest = y[idxtest]\n",
    "    xtest = X[idxtest]\n",
    "    xtrain = scale.fit_transform(xtrain)\n",
    "    xtest = scale.transform(xtest)\n",
    "\n",
    "    #Train and predict for LWR and boosted LWR\n",
    "    yhat_lwr = lw_reg(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
    "    yhat_blwr = boosted_lwr(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
    "\n",
    "    #Train and predict with random forest\n",
    "    model_rf = RandomForestRegressor(n_estimators=100,max_depth=3)\n",
    "    model_rf.fit(xtrain,ytrain)\n",
    "    yhat_rf = model_rf.predict(xtest)\n",
    "\n",
    "    #Train and predict for XGB\n",
    "    model_xgb.fit(xtrain,ytrain)\n",
    "    yhat_xgb = model_xgb.predict(xtest)\n",
    "\n",
    "    #Train and predict for neural network\n",
    "    #Batch size devides the train data into batches, trains on each batch then updates the gradients until it gets through the whole data\n",
    "    #Then moves onto the next epoch\n",
    "    print('Split Number: ' + str(j))\n",
    "    model_nn.fit(xtrain,ytrain,validation_split=0.3, epochs=100, batch_size=20, verbose=0, callbacks=[es])\n",
    "    yhat_nn = model_nn.predict(xtest)\n",
    "\n",
    "    #KernReg training and predictions\n",
    "    model_KernReg = KernelReg(endog = ytrain.reshape(-1,1), exog = xtrain, var_type='ccc', ckertype='gaussian')\n",
    "    yhat_kern, y_kern_std = model_KernReg.fit(xtest) \n",
    "\n",
    "    #Append each model's MSE\n",
    "    mse_lwr.append(mse(ytest,yhat_lwr))\n",
    "    mse_blwr.append(mse(ytest,yhat_blwr))\n",
    "    mse_rf.append(mse(ytest,yhat_rf))\n",
    "    mse_xgb.append(mse(ytest,yhat_xgb))\n",
    "    mse_nn.append(mse(ytest,yhat_nn))\n",
    "    mse_kern.append(mse(ytest,yhat_kern))\n",
    "\n",
    "print('\\n')\n",
    "print('The Cross-validated Mean Squared Error for LWR is : '+str(np.mean(mse_lwr)))\n",
    "print('The Cross-validated Mean Squared Error for BLWR is : '+str(np.mean(mse_blwr)))\n",
    "print('The Cross-validated Mean Squared Error for RF is : '+str(np.mean(mse_rf)))\n",
    "print('The Cross-validated Mean Squared Error for XGB is : '+str(np.mean(mse_xgb)))\n",
    "print('The Cross-validated Mean Squared Error for NN is : '+str(np.mean(mse_nn)))\n",
    "print('The Cross-validated Mean Squared Error for KernReg is : '+str(np.mean(mse_kern)))\n",
    "\n",
    "#dict = {np.mean(mse_lwr) : 'LWR', np.mean(mse_blwr): \"BLWR\", np.mean(mse_rf): \"RFR\", np.mean(mse_xgb): \"XGB\", np.mean(mse_nn): \"NN\", np.mean(mse_kern): \"KernReg\"}\n",
    "#print('The winner is :' + dict[np.min(dict.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated Boosting\n",
    "Update the algorithms we have for repeated boosting.\n",
    "Compare with more boosting algorithms such as LightGBM (and, time allows, Catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosted LWR with decision tree or random forest boosting\n",
    "def rep_boosted_lwr(X, y, xtest, kern, tau, booster, nboost, intercept):\n",
    "\n",
    "    yhat = lw_reg(X,y,X,kern,tau,intercept)\n",
    "    yhat_test = lw_reg(X,y,xtest,kern,tau,intercept)\n",
    "\n",
    "    lw_error = y - yhat\n",
    "    \n",
    "    for i in range(nboost):\n",
    "        booster.fit(X,lw_error)\n",
    "        yhat += booster.predict(X)\n",
    "        yhat_test += booster.predict(xtest)\n",
    "        lw_error = y - yhat\n",
    "\n",
    "    return yhat_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 1234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NELKSH~1\\AppData\\Local\\Temp/ipykernel_17668/2099630893.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m#KernReg training and predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mmodel_KernReg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKernelReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ccc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckertype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gaussian'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0myhat_kern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_kern_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_KernReg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\nonparametric\\kernel_regression.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, var_type, reg_type, bw, ckertype, okertype, ukertype, defaults)\u001b[0m\n\u001b[0;32m    119\u001b[0m                                  'number of variables.')\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mefficient\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_reg_bw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_efficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\nonparametric\\kernel_regression.py\u001b[0m in \u001b[0;36m_compute_reg_bw\u001b[1;34m(self, bw)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             bw_estimated = optimize.fmin(res, x0=h0, args=(func, ),\n\u001b[0m\u001b[0;32m    143\u001b[0m                                          maxiter=1e3, maxfun=1e3, disp=0)\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mbw_estimated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(func, x0, args, xtol, ftol, maxiter, maxfun, full_output, disp, retall, callback, initial_simplex)\u001b[0m\n\u001b[0;32m    578\u001b[0m             'initial_simplex': initial_simplex}\n\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_minimize_neldermead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0mretlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fun'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nfev'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_neldermead\u001b[1;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, bounds, **unknown_options)\u001b[0m\n\u001b[0;32m    803\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mbounds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m                         \u001b[0mxcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxcc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m                     \u001b[0mfxcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxcc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfxcc\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mfsim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(x, *wrapper_args)\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\nonparametric\\kernel_regression.py\u001b[0m in \u001b[0;36mcv_loo\u001b[1;34m(self, bw, func)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_not_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLOO_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLOO_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m             G = func(bw, endog=Y, exog=-X_not_i,\n\u001b[0m\u001b[0;32m    333\u001b[0m                      data_predict=-self.exog[ii, :])[0]\n\u001b[0;32m    334\u001b[0m             \u001b[0mL\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\nonparametric\\kernel_regression.py\u001b[0m in \u001b[0;36m_est_loc_linear\u001b[1;34m(self, bw, endog, exog, data_predict)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mV\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexog\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdata_predict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mker_endog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mmean_mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_mfx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mmfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_mfx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mpinv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mpinv\u001b[1;34m(a, rcond, hermitian)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1992\u001b[0m     \u001b[1;31m# discard small singular values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1993\u001b[1;33m     \u001b[0mcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrcond\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1994\u001b[0m     \u001b[0mlarge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlarge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2789\u001b[0m     \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2790\u001b[0m     \"\"\"\n\u001b[1;32m-> 2791\u001b[1;33m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[0;32m   2792\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   2793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we want more nested cross-validations\n",
    "#This block will take an hour of more (long time)\n",
    "#Neural networks are usually no match for boosted algorithms when it comes to regression on a continuos variable\n",
    "\n",
    "mse_lwr = []\n",
    "mse_blwr = []\n",
    "mse_rf = []\n",
    "mse_xgb = []\n",
    "mse_nn = []\n",
    "mse_kern = []\n",
    "\n",
    "for i in range(1234,1240):\n",
    "  print('Random State: ' + str(i))\n",
    "  kf = KFold(n_splits=10,shuffle=True,random_state=i)\n",
    "  # this is the random state cross-validation loop to make sure our results are real, not just the state being good/bad for a particular model\n",
    "  j = 0\n",
    "  for idxtrain, idxtest in kf.split(X):\n",
    "    j += 1\n",
    "    #Split the train and test data\n",
    "    xtrain = X[idxtrain]\n",
    "    ytrain = y[idxtrain]\n",
    "    ytest = y[idxtest]\n",
    "    xtest = X[idxtest]\n",
    "    xtrain = scale.fit_transform(xtrain)\n",
    "    xtest = scale.transform(xtest)\n",
    "    #print('Split Number: ' + str(j))\n",
    "\n",
    "    #Train and predict for LWR and boosted LWR\n",
    "    yhat_lwr = lw_reg(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
    "    \n",
    "    #Boosted LWR with repeated boosting\n",
    "    booster_rf = RandomForestRegressor(n_estimators=100,max_depth=2)\n",
    "    yhat_blwr = rep_boosted_lwr(xtrain,ytrain,xtest,Tricubic,1.2,booster_rf,2,True)\n",
    "\n",
    "    #Train and predict with random forest\n",
    "    model_rf = RandomForestRegressor(n_estimators=100,max_depth=3)\n",
    "    model_rf.fit(xtrain,ytrain)\n",
    "    yhat_rf = model_rf.predict(xtest)\n",
    "\n",
    "    #Train and predict for XGB\n",
    "    model_xgb.fit(xtrain,ytrain)\n",
    "    yhat_xgb = model_xgb.predict(xtest)\n",
    "\n",
    "    #Train and predict for neural network\n",
    "    #Batch size devides the train data into batches, trains on each batch then updates the gradients until it gets through the whole data\n",
    "    #Then moves onto the next epoch\n",
    "    model_nn.fit(xtrain,ytrain,validation_split=0.3, epochs=100, batch_size=20, verbose=0, callbacks=[es])\n",
    "    yhat_nn = model_nn.predict(xtest)\n",
    "\n",
    "    #KernReg training and predictions\n",
    "    model_KernReg = KernelReg(endog = ytrain.reshape(-1,1), exog = xtrain, var_type='ccc', ckertype='gaussian')\n",
    "    yhat_kern, y_kern_std = model_KernReg.fit(xtest) \n",
    "\n",
    "    #Append each model's MSE\n",
    "    mse_lwr.append(mse(ytest,yhat_lwr))\n",
    "    mse_blwr.append(mse(ytest,yhat_blwr))\n",
    "    mse_rf.append(mse(ytest,yhat_rf))\n",
    "    mse_xgb.append(mse(ytest,yhat_xgb))\n",
    "    mse_nn.append(mse(ytest,yhat_nn))\n",
    "    mse_kern.append(mse(ytest,yhat_kern))\n",
    "\n",
    "print('\\n')\n",
    "print('The Cross-validated Mean Squared Error for LWR is : '+str(np.mean(mse_lwr)))\n",
    "print('The Cross-validated Mean Squared Error for BLWR is : '+str(np.mean(mse_blwr)))\n",
    "print('The Cross-validated Mean Squared Error for RF is : '+str(np.mean(mse_rf)))\n",
    "print('The Cross-validated Mean Squared Error for XGB is : '+str(np.mean(mse_xgb)))\n",
    "print('The Cross-validated Mean Squared Error for NN is : '+str(np.mean(mse_nn)))\n",
    "print('The Cross-validated Mean Squared Error for KernReg is : '+str(np.mean(mse_kern)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb8b7f83ee360ab7c65e3bf584de3c309f966a927bbfa0413b48c366d5d9edbf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
