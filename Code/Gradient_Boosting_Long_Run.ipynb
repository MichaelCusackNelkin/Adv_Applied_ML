{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SOqMlRibziB"
      },
      "source": [
        "# DATA 410 Lecture 10 - Spring 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDb8bGUrbV3Y"
      },
      "source": [
        "<font face=\"Chalkboard\" color=\"darkgreen\" size=10> Multivariate Models for Regression: Gradient Boosting</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1NhfDMt1bVyx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.linalg import lstsq\n",
        "from scipy.sparse.linalg import lsmr\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d, griddata, LinearNDInterpolator, NearestNDInterpolator\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold, train_test_split as tts\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJmTp3nUym6A"
      },
      "source": [
        "## Gradient Boosting\n",
        "\n",
        "Assume you have an regressor $F$ and, for the observation $x_i$ we make the prediction $F(x_i)$. To improve the predictions, we can regard $F$ as a 'weak learner' and therefore train a decision tree (we can call it $h$) where the new output is $y_i-F(x_i)$. Thus, there are increased chances that the new regressor\n",
        "\n",
        "$$\\large F + h$$ \n",
        "\n",
        "is better than the old one, $F.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hy7vVKWy_ijS"
      },
      "outputs": [],
      "source": [
        "# import libraries for creating a neural network\n",
        "# imports for creating a Neural Network\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.optimizers import Adam # they recently updated Tensorflow\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oc9b_REyAEI_"
      },
      "outputs": [],
      "source": [
        "# Tricubic Kernel\n",
        "def Tricubic(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,70/81*(1-d**3)**3)\n",
        "\n",
        "# Quartic Kernel\n",
        "def Quartic(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,15/16*(1-d**2)**2)\n",
        "\n",
        "# Epanechnikov Kernel\n",
        "def Epanechnikov(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,3/4*(1-d**2)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iudc4aEGAXBw"
      },
      "outputs": [],
      "source": [
        "#Defining the kernel local regression model\n",
        "\n",
        "def lw_reg(X, y, xnew, kern, tau, intercept):\n",
        "    # tau is called bandwidth K((x-x[i])/(2*tau))\n",
        "    n = len(X) # the number of observations\n",
        "    yest = np.zeros(n)\n",
        "\n",
        "    if len(y.shape)==1: # here we make column vectors\n",
        "      y = y.reshape(-1,1)\n",
        "\n",
        "    if len(X.shape)==1:\n",
        "      X = X.reshape(-1,1)\n",
        "    \n",
        "    if intercept:\n",
        "      X1 = np.column_stack([np.ones((len(X),1)),X])\n",
        "    else:\n",
        "      X1 = X\n",
        "\n",
        "    w = np.array([kern((X - X[i])/(2*tau)) for i in range(n)]) # here we compute n vectors of weights\n",
        "\n",
        "    #Looping through all X-points\n",
        "    for i in range(n):          \n",
        "        W = np.diag(w[:,i])\n",
        "        b = np.transpose(X1).dot(W).dot(y)\n",
        "        A = np.transpose(X1).dot(W).dot(X1)\n",
        "        #A = A + 0.001*np.eye(X1.shape[1]) # if we want L2 regularization\n",
        "        #theta = linalg.solve(A, b) # A*theta = b\n",
        "        beta, res, rnk, s = lstsq(A, b)\n",
        "        yest[i] = np.dot(X1[i],beta)\n",
        "    if X.shape[1]==1:\n",
        "      f = interp1d(X.flatten(),yest,fill_value='extrapolate')\n",
        "    else:\n",
        "      f = LinearNDInterpolator(X, yest)\n",
        "    output = f(xnew) # the output may have NaN's where the data points from xnew are outside the convex hull of X\n",
        "    if sum(np.isnan(output))>0:\n",
        "      g = NearestNDInterpolator(X,y.ravel()) \n",
        "      # output[np.isnan(output)] = g(X[np.isnan(output)])\n",
        "      output[np.isnan(output)] = g(xnew[np.isnan(output)])\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pVfSPIBqAXEB"
      },
      "outputs": [],
      "source": [
        "def boosted_lwr(X, y, xnew, kern, tau, intercept):\n",
        "  # we need decision trees\n",
        "  # for training the boosted method we use X and y\n",
        "  Fx = lw_reg(X,y,X,kern,tau,intercept) # we need this for training the Decision Tree\n",
        "  # Now train the Decision Tree on y_i - F(x_i)\n",
        "  new_y = y - Fx\n",
        "  #model = DecisionTreeRegressor(max_depth=2, random_state=123)\n",
        "  model = RandomForestRegressor(n_estimators=100,max_depth=2)\n",
        "  #model = model_xgb\n",
        "  model.fit(X,new_y)\n",
        "  output = model.predict(xnew) + lw_reg(X,y,xnew,kern,tau,intercept)\n",
        "  return output "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCGLoGtiAGAb"
      },
      "source": [
        "## Create a \"boosted\" (in the sense of Gradient Boosting) version of the Locally Weighted Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yWMuUAZqAXGH"
      },
      "outputs": [],
      "source": [
        "cars = pd.read_csv('Data/cars.csv')\n",
        "\n",
        "X = cars[['ENG','CYL','WGT']].values\n",
        "y = cars['MPG'].values\n",
        "\n",
        "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
        "scale = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQZl_-2lAXKW",
        "outputId": "3d320272-b7df-46a5-e28b-f7058181862a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Cross-validated Mean Squared Error for LWR is : 16.931090772336\n",
            "The Cross-validated Mean Squared Error for BLWR is : 16.689854441790565\n",
            "The Cross-validated Mean Squared Error for RF is : 16.61211737943362\n"
          ]
        }
      ],
      "source": [
        "mse_lwr = []\n",
        "mse_blwr = []\n",
        "mse_rf = []\n",
        "mse_nn = []\n",
        "\n",
        "# this is the Cross-Validation Loop\n",
        "for idxtrain, idxtest in kf.split(X):\n",
        "  xtrain = X[idxtrain]\n",
        "  ytrain = y[idxtrain]\n",
        "  ytest = y[idxtest]\n",
        "  xtest = X[idxtest]\n",
        "  xtrain = scale.fit_transform(xtrain)\n",
        "  xtest = scale.transform(xtest)\n",
        "  yhat_lwr = lw_reg(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
        "  yhat_blwr = boosted_lwr(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
        "  model_rf = RandomForestRegressor(n_estimators=100,max_depth=3)\n",
        "  model_rf.fit(xtrain,ytrain)\n",
        "  yhat_rf = model_rf.predict(xtest)\n",
        "  #model_nn.fit(xtrain,ytrain,validation_split=0.3, epochs=500, batch_size=20, verbose=0, callbacks=[es])\n",
        "  #yhat_nn = model_nn.predict(xtest)\n",
        "  mse_lwr.append(mse(ytest,yhat_lwr))\n",
        "  mse_blwr.append(mse(ytest,yhat_blwr))\n",
        "  mse_rf.append(mse(ytest,yhat_rf))\n",
        "  #mse_nn.append(mse(ytest,yhat_nn))\n",
        "print('The Cross-validated Mean Squared Error for LWR is : '+str(np.mean(mse_lwr)))\n",
        "print('The Cross-validated Mean Squared Error for BLWR is : '+str(np.mean(mse_blwr)))\n",
        "print('The Cross-validated Mean Squared Error for RF is : '+str(np.mean(mse_rf)))\n",
        "#print('The Cross-validated Mean Squared Error for NN is : '+str(np.mean(mse_nn)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yeFywEpEXPas"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiQhS7GZAXOm",
        "outputId": "111e4f49-93ff-4925-93be-32b6e610f843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Cross-validated Mean Squared Error for LWR is : 16.901595996974294\n",
            "The Cross-validated Mean Squared Error for BLWR is : 16.664210633900517\n",
            "The Cross-validated Mean Squared Error for RF is : 16.90177994216944\n",
            "The Cross-validated Mean Squared Error for XGB is : 16.25007368020466\n"
          ]
        }
      ],
      "source": [
        "# we want more nested cross-validations\n",
        "#This block will take an hour of more (long time)\n",
        "\n",
        "mse_lwr = []\n",
        "mse_blwr = []\n",
        "mse_rf = []\n",
        "mse_xgb = []\n",
        "mse_nn = []\n",
        "for i in range(1):\n",
        "  kf = KFold(n_splits=10,shuffle=True,random_state=i)\n",
        "  # this is the Cross-Validation Loop\n",
        "  for idxtrain, idxtest in kf.split(X):\n",
        "    xtrain = X[idxtrain]\n",
        "    ytrain = y[idxtrain]\n",
        "    ytest = y[idxtest]\n",
        "    xtest = X[idxtest]\n",
        "    xtrain = scale.fit_transform(xtrain)\n",
        "    xtest = scale.transform(xtest)\n",
        "    yhat_lwr = lw_reg(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
        "    yhat_blwr = boosted_lwr(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
        "    model_rf = RandomForestRegressor(n_estimators=100,max_depth=3)\n",
        "    model_rf.fit(xtrain,ytrain)\n",
        "    yhat_rf = model_rf.predict(xtest)\n",
        "    model_xgb.fit(xtrain,ytrain)\n",
        "    yhat_xgb = model_xgb.predict(xtest)\n",
        "    #model_nn.fit(xtrain,ytrain,validation_split=0.3, epochs=500, batch_size=20, verbose=0, callbacks=[es])\n",
        "    #yhat_nn = model_nn.predict(xtest)\n",
        "    mse_lwr.append(mse(ytest,yhat_lwr))\n",
        "    mse_blwr.append(mse(ytest,yhat_blwr))\n",
        "    mse_rf.append(mse(ytest,yhat_rf))\n",
        "    mse_xgb.append(mse(ytest,yhat_xgb))\n",
        "    #mse_nn.append(mse(ytest,yhat_nn))\n",
        "print('The Cross-validated Mean Squared Error for LWR is : '+str(np.mean(mse_lwr)))\n",
        "print('The Cross-validated Mean Squared Error for BLWR is : '+str(np.mean(mse_blwr)))\n",
        "print('The Cross-validated Mean Squared Error for RF is : '+str(np.mean(mse_rf)))\n",
        "print('The Cross-validated Mean Squared Error for XGB is : '+str(np.mean(mse_xgb)))\n",
        "#print('The Cross-validated Mean Squared Error for NN is : '+str(np.mean(mse_nn)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_uDLN75b2qH"
      },
      "source": [
        "## Here are the predictions we made for the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUbUsyGh-nRw",
        "outputId": "3663bd78-ad17-4dbd-90ca-bf4ca8a8838b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE Neural Network = $2,652.57\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "yhat_nn = model.predict(dat_test[:,:-1])\n",
        "mae_nn = mean_absolute_error(dat_test[:,-1], yhat_nn)\n",
        "print(\"MAE Neural Network = ${:,.2f}\".format(1000*mae_nn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-fji3Nt-owj"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,9))\n",
        "ax.set_xlim(3, 9)\n",
        "ax.set_ylim(0, 51)\n",
        "ax.scatter(x=df['rooms'], y=df['cmedv'],s=25)\n",
        "ax.plot(X_test, lm.predict(X_test), color='red',label='Linear Regression')\n",
        "ax.plot(dat_test[:,0], yhat_nn, color='lightgreen',lw=2.5,label='Neural Network')\n",
        "ax.plot(dat_test[:,0], model_lowess(dat_train,dat_test,Epanechnikov,0.53), color='orange',lw=2.5,label='Kernel Weighted Regression')\n",
        "ax.set_xlabel('Number of Rooms',fontsize=16,color='navy')\n",
        "ax.set_ylabel('House Price (Thousands of Dollars)',fontsize=16,color='navy')\n",
        "ax.set_title('Boston Housing Prices',fontsize=16,color='purple')\n",
        "ax.grid(b=True,which='major', color ='grey', linestyle='-', alpha=0.8)\n",
        "ax.grid(b=True,which='minor', color ='grey', linestyle='--', alpha=0.2)\n",
        "ax.minorticks_on()\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqzyt7BARDCl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=1693)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWdGHA9TRHVw",
        "outputId": "ac7dbdaf-d274-45fb-8617-742ad3241d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validated MAE Linear Regression = $4,447.94\n"
          ]
        }
      ],
      "source": [
        "mae_lm = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  X_train = dat[idxtrain,0]\n",
        "  y_train = dat[idxtrain,1]\n",
        "  X_test  = dat[idxtest,0]\n",
        "  y_test = dat[idxtest,1]\n",
        "  lm.fit(X_train.reshape(-1,1),y_train)\n",
        "  yhat_lm = lm.predict(X_test.reshape(-1,1))\n",
        "  mae_lm.append(mean_absolute_error(y_test, yhat_lm))\n",
        "print(\"Validated MAE Linear Regression = ${:,.2f}\".format(1000*np.mean(mae_lm)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1yCP16cgFSz",
        "outputId": "68768f49-ad4a-4f61-ed46-e686cdddd663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validated MAE Local Kernel Regression = $4,090.03\n",
            "Validated MAE Local Kernel Regression = $4,090.03\n",
            "Validated MAE Local Kernel Regression = $4,090.03\n",
            "1 loop, best of 3: 531 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "\n",
        "mae_lk = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  dat_test = dat[idxtest,:]\n",
        "  y_test = dat_test[np.argsort(dat_test[:, 0]),1]\n",
        "  yhat_lk = model_lowess(dat[idxtrain,:],dat[idxtest,:],Gaussian,0.15)\n",
        "  mae_lk.append(mean_absolute_error(y_test, yhat_lk))\n",
        "print(\"Validated MAE Local Kernel Regression = ${:,.2f}\".format(1000*np.mean(mae_lk)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_CUmGMZR0Tk",
        "outputId": "456934f3-024b-4d5e-d6e8-a6f81407beed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00501: early stopping\n",
            "Epoch 00506: early stopping\n",
            "Epoch 00539: early stopping\n",
            "Epoch 00800: early stopping\n",
            "Epoch 00672: early stopping\n",
            "Epoch 00689: early stopping\n",
            "Epoch 00551: early stopping\n",
            "Validated MAE Neural Network Regression = $5,108.02\n"
          ]
        }
      ],
      "source": [
        "#%%timeit -n 1\n",
        "\n",
        "mae_nn = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  X_train = dat[idxtrain,0]\n",
        "  y_train = dat[idxtrain,1]\n",
        "  X_test  = dat[idxtest,0]\n",
        "  y_test = dat[idxtest,1]\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=500)\n",
        "  model.fit(X_train.reshape(-1,1),y_train,validation_split=0.3, epochs=1000, batch_size=100, verbose=0, callbacks=[es])\n",
        "  yhat_nn = model.predict(X_test.reshape(-1,1))\n",
        "  mae_nn.append(mean_absolute_error(y_test, yhat_nn))\n",
        "print(\"Validated MAE Neural Network Regression = ${:,.2f}\".format(1000*np.mean(mae_nn)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mom_T9x8bUHl"
      },
      "source": [
        "## XGBoost\n",
        "\n",
        "The method is related to Random Forest\n",
        "\n",
        "https://towardsdatascience.com/xgboost-python-example-42777d01001e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwEh4jc3wKyw"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO9TMxFBwcen",
        "outputId": "b346e654-36ba-4bf1-ef71-ec11c0806790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validated MAE XGBoost Regression = $4,179.17\n",
            "Validated MAE XGBoost Regression = $4,179.17\n",
            "Validated MAE XGBoost Regression = $4,179.17\n",
            "1 loop, best of 3: 181 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "\n",
        "mae_xgb = []\n",
        "\n",
        "for idxtrain, idxtest in kf.split(dat):\n",
        "  X_train = dat[idxtrain,0]\n",
        "  y_train = dat[idxtrain,1]\n",
        "  X_test  = dat[idxtest,0]\n",
        "  y_test = dat[idxtest,1]\n",
        "  model_xgb.fit(X_train.reshape(-1,1),y_train)\n",
        "  yhat_xgb = model_xgb.predict(X_test.reshape(-1,1))\n",
        "  mae_xgb.append(mean_absolute_error(y_test, yhat_xgb))\n",
        "print(\"Validated MAE XGBoost Regression = ${:,.2f}\".format(1000*np.mean(mae_xgb)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk4DCf7-rzEy"
      },
      "outputs": [],
      "source": [
        "cstring = 'c'*364"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQyF8aP3r6_"
      },
      "source": [
        "## Using Kernel Regression from StatsModels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZhrdVXTs9mL"
      },
      "outputs": [],
      "source": [
        "hello = 'c'*dat_train_poly.shape[1]\n",
        "hello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP_wvqM9thfi"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "dat_train_poly = poly.fit_transform(dat_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obBU-bGuwsKq"
      },
      "outputs": [],
      "source": [
        "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
        "\n",
        "\n",
        "model_KernReg = KernelReg(endog=dat_train[:,-1],exog=dat_train_poly,var_type=hello,ckertype='gaussian')\n",
        "yhat_sm_test, y_std = model_KernReg.fit(dat_test[:,:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geqRpN8siZ1W",
        "outputId": "0842b17b-6a46-469e-e4b9-e731221a5ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE StatsModels Kernel Regression = $2,854.57\n"
          ]
        }
      ],
      "source": [
        "mae_sm = mean_absolute_error(dat_test[:,-1], yhat_sm_test)\n",
        "print(\"MAE StatsModels Kernel Regression = ${:,.2f}\".format(1000*mae_sm))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "DATA 410  Lecture 10 - Spring 2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
