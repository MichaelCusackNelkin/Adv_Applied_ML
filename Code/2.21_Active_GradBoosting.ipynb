{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import lstsq\n",
    "from scipy.sparse.linalg import lsmr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d, griddata, LinearNDInterpolator, NearestNDInterpolator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split as tts\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# import libraries for creating a neural network\n",
    "# imports for creating a Neural Network\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop # they recently updated Tensorflow\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#XGB Boosting\n",
    "import xgboost as xgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tricubic Kernel\n",
    "def Tricubic(x):\n",
    "  if len(x.shape) == 1:\n",
    "    x = x.reshape(-1,1)\n",
    "  d = np.sqrt(np.sum(x**2,axis=1))\n",
    "  return np.where(d>1,0,70/81*(1-d**3)**3)\n",
    "\n",
    "# Quartic Kernel\n",
    "def Quartic(x):\n",
    "  if len(x.shape) == 1:\n",
    "    x = x.reshape(-1,1)\n",
    "  d = np.sqrt(np.sum(x**2,axis=1))\n",
    "  return np.where(d>1,0,15/16*(1-d**2)**2)\n",
    "\n",
    "# Epanechnikov Kernel\n",
    "def Epanechnikov(x):\n",
    "  if len(x.shape) == 1:\n",
    "    x = x.reshape(-1,1)\n",
    "  d = np.sqrt(np.sum(x**2,axis=1))\n",
    "  return np.where(d>1,0,3/4*(1-d**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the kernel local regression model\n",
    "\n",
    "def lw_reg(X, y, xnew, kern, tau, intercept):\n",
    "    # tau is called bandwidth K((x-x[i])/(2*tau))\n",
    "    n = len(X) # the number of observations\n",
    "    yest = np.zeros(n)\n",
    "\n",
    "    if len(y.shape)==1: # here we make column vectors\n",
    "      y = y.reshape(-1,1)\n",
    "\n",
    "    if len(X.shape)==1:\n",
    "      X = X.reshape(-1,1)\n",
    "    \n",
    "    if intercept:\n",
    "      X1 = np.column_stack([np.ones((len(X),1)),X])\n",
    "    else:\n",
    "      X1 = X\n",
    "\n",
    "    w = np.array([kern((X - X[i])/(2*tau)) for i in range(n)]) # here we compute n vectors of weights\n",
    "\n",
    "    #Looping through all X-points\n",
    "    for i in range(n):          \n",
    "        W = np.diag(w[:,i])\n",
    "        b = np.transpose(X1).dot(W).dot(y)\n",
    "        A = np.transpose(X1).dot(W).dot(X1)\n",
    "        #A = A + 0.001*np.eye(X1.shape[1]) # if we want L2 regularization\n",
    "        #theta = linalg.solve(A, b) # A*theta = b\n",
    "        beta, res, rnk, s = lstsq(A, b)\n",
    "        yest[i] = np.dot(X1[i],beta)\n",
    "    if X.shape[1]==1:\n",
    "      f = interp1d(X.flatten(),yest,fill_value='extrapolate')\n",
    "    else:\n",
    "      f = LinearNDInterpolator(X, yest)\n",
    "    output = f(xnew) # the output may have NaN's where the data points from xnew are outside the convex hull of X\n",
    "    if sum(np.isnan(output))>0:\n",
    "      g = NearestNDInterpolator(X,y.ravel()) \n",
    "      # output[np.isnan(output)] = g(X[np.isnan(output)])\n",
    "      output[np.isnan(output)] = g(xnew[np.isnan(output)])\n",
    "    return output\n",
    "\n",
    "#Boosted LWR with decision tree or random forest boosting\n",
    "def boosted_lwr(X, y, xnew, kern, tau, intercept): \n",
    "  # we need decision trees\n",
    "  # for training the boosted method we use X and y\n",
    "  Fx = lw_reg(X,y,X,kern,tau,intercept) # we need this for training the Decision Tree\n",
    "  # Now train the Decision Tree on y_i - F(x_i)\n",
    "  new_y = y - Fx\n",
    "  #model = DecisionTreeRegressor(max_depth=2, random_state=123)\n",
    "  model = RandomForestRegressor(n_estimators=100,max_depth=2)\n",
    "  #model = model_xgb\n",
    "  model.fit(X,new_y)\n",
    "  output = model.predict(xnew) + lw_reg(X,y,xnew,kern,tau,intercept)\n",
    "  return output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we design a Nerual Network for regression\n",
    "\n",
    "How many layers? How many neurons in each layer? All dense layers (MLP)? What other types? Which activation functions do we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('Data/cars.csv')\n",
    "\n",
    "X = cars[['ENG','CYL','WGT']].values\n",
    "y = cars['MPG'].values\n",
    "\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
    "scale = StandardScaler()\n",
    "\n",
    "data = np.concatenate([X,y.reshape(-1,1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = Sequential() #Making a tensorflow sequential network\n",
    "\n",
    "#Arbitrary number of layers, try 5? Lets do 128-neuron dense layers with relu activation layers\n",
    "model_nn.add(Dense(128, activation=\"relu\", input_dim=3))\n",
    "model_nn.add(Dropout(0.1))\n",
    "model_nn.add(Dense(64, activation=\"relu\"))\n",
    "model_nn.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "#Doing a linear regression, so linear activation in the last layer (and only one output dim 1)\n",
    "#It's solving a regression problem, so it's predicting a continuous random variable, meaning the activation of the last\n",
    "#neuron is the continuous random variable to be predicted.\n",
    "model_nn.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "#Optimizer is doing gradient descent on the weights and biases and updating them via backpropogation\n",
    "model_nn.compile(loss='mean_squared_error', optimizer=Adam(learning_rate= 0.001, decay = 0.0001))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=600) #Stops the training loop if it's taking too long(gets stuck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The XGB model definition\n",
    "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold Validation Number: 1\n",
      "\n",
      "Split Number: 1\n",
      "Split Number: 2\n",
      "Split Number: 3\n",
      "Split Number: 4\n",
      "Split Number: 5\n",
      "Split Number: 6\n",
      "Split Number: 7\n",
      "Split Number: 8\n",
      "Split Number: 9\n",
      "Split Number: 10\n",
      "\n",
      " The Cross-validated Mean Squared Error for LWR is : 16.901595996974294\n",
      "The Cross-validated Mean Squared Error for BLWR is : 16.64113938021411\n",
      "The Cross-validated Mean Squared Error for RF is : 16.77570050694267\n",
      "The Cross-validated Mean Squared Error for XGB is : 16.25007368020466\n",
      "The Cross-validated Mean Squared Error for NN is : 21.709172153294126\n"
     ]
    }
   ],
   "source": [
    "# we want more nested cross-validations\n",
    "#This block will take an hour of more (long time)\n",
    "#Neural networks are usually no match for boosted algorithms when it comes to regression on a continuos variable\n",
    "\n",
    "mse_lwr = []\n",
    "mse_blwr = []\n",
    "mse_rf = []\n",
    "mse_xgb = []\n",
    "mse_nn = []\n",
    "\n",
    "for i in range(1):\n",
    "  print('KFold Validation Number: ' + str(i+1) + '\\n')\n",
    "  kf = KFold(n_splits=10,shuffle=True,random_state=i)\n",
    "  # this is the Cross-Validation Loop\n",
    "  j = 0\n",
    "  for idxtrain, idxtest in kf.split(X):\n",
    "    j += 1\n",
    "    #Split the train and test data\n",
    "    xtrain = X[idxtrain]\n",
    "    ytrain = y[idxtrain]\n",
    "    ytest = y[idxtest]\n",
    "    xtest = X[idxtest]\n",
    "    xtrain = scale.fit_transform(xtrain)\n",
    "    xtest = scale.transform(xtest)\n",
    "\n",
    "    #Train and predict for LWR and boosted LWR\n",
    "    yhat_lwr = lw_reg(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
    "    yhat_blwr = boosted_lwr(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
    "\n",
    "    #Train and predict with random forest\n",
    "    model_rf = RandomForestRegressor(n_estimators=100,max_depth=3)\n",
    "    model_rf.fit(xtrain,ytrain)\n",
    "    yhat_rf = model_rf.predict(xtest)\n",
    "\n",
    "    #Train and predict for XGB\n",
    "    model_xgb.fit(xtrain,ytrain)\n",
    "    yhat_xgb = model_xgb.predict(xtest)\n",
    "\n",
    "    #Train and predict for neural network\n",
    "    #Batch size devides the train data into batches, trains on each batch then updates the gradients until it gets through the whole data\n",
    "    #Then moves onto the next epoch\n",
    "    print('Split Number: ' + str(j))\n",
    "    model_nn.fit(xtrain,ytrain,validation_split=0.3, epochs=100, batch_size=20, verbose=0, callbacks=[es])\n",
    "    yhat_nn = model_nn.predict(xtest)\n",
    "\n",
    "    #Append each model's MSE\n",
    "    mse_lwr.append(mse(ytest,yhat_lwr))\n",
    "    mse_blwr.append(mse(ytest,yhat_blwr))\n",
    "    mse_rf.append(mse(ytest,yhat_rf))\n",
    "    mse_xgb.append(mse(ytest,yhat_xgb))\n",
    "    mse_nn.append(mse(ytest,yhat_nn))\n",
    "\n",
    "print('\\n The Cross-validated Mean Squared Error for LWR is : '+str(np.mean(mse_lwr)))\n",
    "print('The Cross-validated Mean Squared Error for BLWR is : '+str(np.mean(mse_blwr)))\n",
    "print('The Cross-validated Mean Squared Error for RF is : '+str(np.mean(mse_rf)))\n",
    "print('The Cross-validated Mean Squared Error for XGB is : '+str(np.mean(mse_xgb)))\n",
    "print('The Cross-validated Mean Squared Error for NN is : '+str(np.mean(mse_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next apply this to boston housing dataset and concrete strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feb 23rd -- Nadaraya-Watson Estimator\n",
    "\n",
    "Generally, we provide an estimation of Y based on X being a specific x, written $m(x) = E[Y | X=x]$\n",
    "\n",
    "For Nadaraya-Watson estimators, we use $m(x) = E[Y|X=x] = \\int y f_{Y|X=x}(y)dy = \\frac{\\int y f(x,y)dy}{f_{X}(x)}$\n",
    "\n",
    "So basically, the regression function m(x) can be estimated by joint probablity density function of X and Y, called $f$, and \"the marginal\" $f_x$\n",
    "\n",
    "This becomes:\n",
    "\n",
    "$\\hat f(x,y;h) = \\frac{1}{n} \\sum_{i=1}^n K_{h_1}(x-X_i) K_{h_2}(y-Y_i)$ where $K_{h_1}, K_{h_2}$ are the bandwiths in X and Y (like tau for loess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "model_KernReg = KernelReg(endog = data[:,-1], exog = data[:,:-1], var_type='ccc', ckertype='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.333316197606733\n"
     ]
    }
   ],
   "source": [
    "yhat, ystd = model_KernReg.fit(data[:,:-1]) #predictions\n",
    "mse_kern = mse(y,yhat)\n",
    "print(mse_kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold Validation Number: 1\n",
      "\n",
      "Split Number: 1\n",
      "Split Number: 2\n",
      "\n",
      "\n",
      "The Cross-validated Mean Squared Error for LWR is : 18.424762629746425\n",
      "The Cross-validated Mean Squared Error for BLWR is : 19.12014119219971\n",
      "The Cross-validated Mean Squared Error for RF is : 17.636823308639023\n",
      "The Cross-validated Mean Squared Error for XGB is : 17.88009879064699\n",
      "The Cross-validated Mean Squared Error for NN is : 20.20466981812718\n",
      "The Cross-validated Mean Squared Error for KernReg is : 17.73572469703669\n"
     ]
    }
   ],
   "source": [
    "# we want more nested cross-validations\n",
    "#This block will take an hour of more (long time)\n",
    "#Neural networks are usually no match for boosted algorithms when it comes to regression on a continuos variable\n",
    "\n",
    "mse_lwr = []\n",
    "mse_blwr = []\n",
    "mse_rf = []\n",
    "mse_xgb = []\n",
    "mse_nn = []\n",
    "mse_kern = []\n",
    "\n",
    "for i in range(1):\n",
    "  print('KFold Validation Number: ' + str(i+1) + '\\n')\n",
    "  kf = KFold(n_splits=2,shuffle=True,random_state=i)\n",
    "  # this is the Cross-Validation Loop\n",
    "  j = 0\n",
    "  for idxtrain, idxtest in kf.split(X):\n",
    "    j += 1\n",
    "    #Split the train and test data\n",
    "    xtrain = X[idxtrain]\n",
    "    ytrain = y[idxtrain]\n",
    "    ytest = y[idxtest]\n",
    "    xtest = X[idxtest]\n",
    "    xtrain = scale.fit_transform(xtrain)\n",
    "    xtest = scale.transform(xtest)\n",
    "\n",
    "    #Train and predict for LWR and boosted LWR\n",
    "    yhat_lwr = lw_reg(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
    "    yhat_blwr = boosted_lwr(xtrain,ytrain, xtest,Tricubic,tau=1.2,intercept=True)\n",
    "\n",
    "    #Train and predict with random forest\n",
    "    model_rf = RandomForestRegressor(n_estimators=100,max_depth=3)\n",
    "    model_rf.fit(xtrain,ytrain)\n",
    "    yhat_rf = model_rf.predict(xtest)\n",
    "\n",
    "    #Train and predict for XGB\n",
    "    model_xgb.fit(xtrain,ytrain)\n",
    "    yhat_xgb = model_xgb.predict(xtest)\n",
    "\n",
    "    #Train and predict for neural network\n",
    "    #Batch size devides the train data into batches, trains on each batch then updates the gradients until it gets through the whole data\n",
    "    #Then moves onto the next epoch\n",
    "    print('Split Number: ' + str(j))\n",
    "    model_nn.fit(xtrain,ytrain,validation_split=0.3, epochs=100, batch_size=20, verbose=0, callbacks=[es])\n",
    "    yhat_nn = model_nn.predict(xtest)\n",
    "\n",
    "    #KernReg training and predictions\n",
    "    model_KernReg = KernelReg(endog = ytrain.reshape(-1,1), exog = xtrain, var_type='ccc', ckertype='gaussian')\n",
    "    yhat_kern, y_kern_std = model_KernReg.fit(xtest) \n",
    "\n",
    "    #Append each model's MSE\n",
    "    mse_lwr.append(mse(ytest,yhat_lwr))\n",
    "    mse_blwr.append(mse(ytest,yhat_blwr))\n",
    "    mse_rf.append(mse(ytest,yhat_rf))\n",
    "    mse_xgb.append(mse(ytest,yhat_xgb))\n",
    "    mse_nn.append(mse(ytest,yhat_nn))\n",
    "    mse_kern.append(mse(ytest,yhat_kern))\n",
    "\n",
    "dict = {np.mean(mse_lwr) : 'LWR', np.mean(mse_blwr): \"BLWR\", np.mean(mse_rf): \"RFR\", np.mean(mse_xgb): \"XGB\",\n",
    "                              np.mean(mse_nn): \"NN\", np.mean(mse_kern): \"KernReg\"}\n",
    "\n",
    "print('\\n')\n",
    "print('The Cross-validated Mean Squared Error for LWR is : '+str(np.mean(mse_lwr)))\n",
    "print('The Cross-validated Mean Squared Error for BLWR is : '+str(np.mean(mse_blwr)))\n",
    "print('The Cross-validated Mean Squared Error for RF is : '+str(np.mean(mse_rf)))\n",
    "print('The Cross-validated Mean Squared Error for XGB is : '+str(np.mean(mse_xgb)))\n",
    "print('The Cross-validated Mean Squared Error for NN is : '+str(np.mean(mse_nn)))\n",
    "print('The Cross-validated Mean Squared Error for KernReg is : '+str(np.mean(mse_kern)))\n",
    "\n",
    "#print('The winner is :' + dict[np.min(dict.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb8b7f83ee360ab7c65e3bf584de3c309f966a927bbfa0413b48c366d5d9edbf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
