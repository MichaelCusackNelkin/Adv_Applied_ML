{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SFDBZHaDc67f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.dpi'] = 120\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "erf_wWIndDDz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLlqJfnVE-1T"
      },
      "source": [
        "# Gradient Descent Algorithm\n",
        "\n",
        "We can think of a function of two or more variables. We have an objective function e.g. $$\\large L(\\beta_1, \\beta_2)$$\n",
        "\n",
        "We hope to share some ideas when $$L(\\vec{\\beta})$$ is convex in the vector $\\vec{\\beta}$\n",
        "\n",
        "In particular we are interested in the situation when $L$ is the regularized sum of squarred errors, such as:\n",
        "\n",
        "$$L: = \\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (y_i-\\sum_{j=1}^{p} x_{ij} \\cdot \\beta_j)^2 +P_{\\alpha}(\\beta)$$\n",
        "\n",
        "For example consider the Elastic Net:\n",
        "\n",
        "$$P_{\\alpha}(\\beta) = \\alpha \\cdot (\\lambda\\cdot \\sum |\\beta_j| + (1-\\lambda ) \\cdot \\sum (\\beta_j^2))$$\n",
        "\n",
        "We can compute the partial derivatives with repect to $\\beta_j$:\n",
        "\n",
        "$$\\large \\frac{\\partial L}{\\partial \\beta_j}=-2\\cdot\\displaystyle\\sum_{i=1}^{n} (y_i-\\sum_{j=1}^{p} x_{ij} \\cdot \\beta_j)\\cdot (x_{ij}) + \\alpha\\cdot\\lambda\\cdot\\text{sign}(\\beta_j) + \\alpha\\cdot (1-\\lambda)\\cdot 2\\cdot\\beta_j$$\n",
        "\n",
        "In sklearn you provide the L1_ratio that is $$\\frac{\\lambda}{1-\\lambda}$$\n",
        "We want to know how to update $\\vec{\\beta}$ so that we make the best progress in minimizing $L.$ We can think updating $\\vec{\\beta}$ by going $t$ units in a direction $\\vec{v}$. So we ask, what is $\\vec{v}$ that makes the best progress?\n",
        "\n",
        "We consider $g(t):= L(\\vec{\\beta} + t \\cdot \\vec{v})$ SO the derivative of $g$ is\n",
        "\n",
        "$$\\large g'(t)= \\nabla L \\cdot \\vec{v}$$ so for the most dramatic decrease of the objective we need $\\vec{v}=-\\nabla L$\n",
        "\n",
        "<font color='red' size=5pt>This works well if $L$ is convex in $\\beta$ and the only issue would when there is a very shallow basin of the minimum for $L.$</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM2brO75YCA_"
      },
      "source": [
        "The main goal is to approximate a ground truth coefficient vector in probability.\n",
        "\n",
        "$$\\large y = X\\cdot \\beta^* +\\text{noise}$$\n",
        "\n",
        "We can see the noise as $$\\large \\sigma\\cdot \\epsilon$$\n",
        "\n",
        "So we see that:\n",
        "\n",
        "$$\\large y - X\\beta^*=\\sigma \\cdot \\epsilon$$ \n",
        "\n",
        "Now think about taking a partial derivative with respect to $\\beta_j$ for \n",
        "\n",
        "$$\\|y-X\\beta\\|_2$$\n",
        "\n",
        "and we get \n",
        "\n",
        "$$- \\frac{(y-X\\beta)h_j(X)}{\\|y-X\\beta\\|_2}$$\n",
        "\n",
        "This suggests that we should rather minimize $\\|y-X\\beta\\|_2$ + a regularization term\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXGzsD1Peqc7"
      },
      "source": [
        "## Good reads about soft-thresholding:\n",
        "\n",
        "https://www.kaggle.com/residentmario/soft-thresholding-with-lasso-regression\n",
        "\n",
        "https://eeweb.engineering.nyu.edu/iselesni/lecture_notes/SoftThresholding.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "kDQ1xH4Cj2b4"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('drive/MyDrive/Data Sets/concrete.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "cw3mrqLbj2kN"
      },
      "outputs": [],
      "source": [
        "x = data.loc[:,'cement':'age'].values\n",
        "y = data['strength'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "9cMBaixXmm5q"
      },
      "outputs": [],
      "source": [
        "scale = StandardScaler()\n",
        "xscaled = scale.fit_transform(x)\n",
        "y = y - np.mean(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "hvmr2JS8k62r"
      },
      "outputs": [],
      "source": [
        "def gradient_mse(beta, x, y, alpha,l): # we defined a function that computes the gradient of the objective function\n",
        "    n = len(y) # the number of observations\n",
        "    y_hat = x.dot(beta).flatten()\n",
        "    error = (y - y_hat)\n",
        "    mse = (1.0 /n) * np.sum(np.square(error)) + alpha*(l*np.sum(np.abs(beta))+(1-l)*np.sum(beta**2)) # here we have the ridge penalty\n",
        "    gradient = -(2.0 /n) * error.dot(x) + 2*(1-l)*alpha*beta+alpha*l*np.sign(beta) # the penalty is baked into the gradient as well\n",
        "    return gradient, mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "F_uQm7o3o0ok"
      },
      "outputs": [],
      "source": [
        "# data & hyper-parameters\n",
        "alpha = 1\n",
        "l = 1\n",
        "x = xscaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "YCdXXWJhwxlU"
      },
      "outputs": [],
      "source": [
        "beta = np.zeros(x.shape[1])\n",
        "lr = .01 # the learning rate\n",
        "tolerance = 1e-8 # this would be our stopping criteria\n",
        "alpha = 1\n",
        "l = 1\n",
        "old_w = []\n",
        "mse = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNSpWhPCwuTz",
        "outputId": "b60601b7-9b63-4129-ab82-f5ae5e781fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 1000 - Mean Squarred Error: 177.3090\n",
            "The Gradient Descent Algorithm has converged\n",
            "beta = [ 4.85133422  1.97685324 -0.03399281 -3.07841259  2.79580192 -1.00710962\n",
            " -1.64113454  4.04015831]\n"
          ]
        }
      ],
      "source": [
        "# Perform Gradient Descent\n",
        "iterations = 1\n",
        "for i in range(10000):\n",
        "    gradient, mse_temp = gradient_mse(beta, xscaled, y, alpha,0.5)\n",
        "    new_beta = beta - lr * gradient # here we update the coefficients in the direction of the negative gradient\n",
        "    # Print error every 10 iterations\n",
        "    if iterations % 1000 == 0:\n",
        "        print(\"Iteration: %d - Mean Squarred Error: %.4f\" % (iterations, mse_temp))\n",
        "        old_w.append(new_beta)\n",
        "        mse.append(mse_temp)\n",
        " \n",
        "    # Stopping Condition\n",
        "    if np.sum(abs(new_beta - beta)) < tolerance:\n",
        "        print('The Gradient Descent Algorithm has converged')\n",
        "        break\n",
        " \n",
        "    iterations += 1\n",
        "    beta = new_beta\n",
        " \n",
        "print('beta =', beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "QCLRVTHPxw3C"
      },
      "outputs": [],
      "source": [
        "beta = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "Yvw-Qg4YlscH"
      },
      "outputs": [],
      "source": [
        "def objective(beta):\n",
        "  n = len(y) # the number of observations\n",
        "  y_hat = x.dot(beta).flatten()\n",
        "  error = (y - y_hat)\n",
        "  mse = (1.0 /n) * np.sum(np.square(error)) + alpha*(l*np.sum(np.abs(beta))+(1-l)*np.sum(beta**2))\n",
        "  return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "4prG08famDsG"
      },
      "outputs": [],
      "source": [
        "def gradient(beta):\n",
        "  n = len(y) # the number of observations\n",
        "  y_hat = x.dot(beta).flatten()\n",
        "  error = (y - y_hat)\n",
        "  gradient = -(2.0 /n) * error.dot(x) + 2*(1-l)*alpha*beta+alpha*l*np.sign(beta)\n",
        "  return gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "zNNZ6ed9mYt4"
      },
      "outputs": [],
      "source": [
        "b0 = np.zeros(x.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "eS21y4wQk65S"
      },
      "outputs": [],
      "source": [
        "output = minimize(objective, b0, method='L-BFGS-B', jac=gradient,options={'gtol': 1e-8, 'maxiter': 50000,'maxls': 25,'disp': True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC4pJ0PWk67n",
        "outputId": "cfe287c2-73b8-4309-e01f-175d045afefe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 9.15184243e+00,  5.55053209e+00,  2.26544892e+00, -3.86187813e+00,\n",
              "        2.39005425e+00, -1.43391302e-07, -1.50090905e-01,  6.34592712e+00])"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7KGxxxjgQTp",
        "outputId": "078f47fd-720c-4587-d277-8a536d387241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 9.15184243e+00,  5.55053209e+00,  2.26544892e+00, -3.86187813e+00,\n",
              "        2.39005425e+00, -1.43391320e-07, -1.50090905e-01,  6.34592712e+00])"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5Ubb6kPk7AJ",
        "outputId": "1795bb84-576c-4e84-e780-d2f94219914d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 9.15184243e+00,  5.55053209e+00,  2.26544892e+00, -3.86187813e+00,\n",
              "        2.39005425e+00, -1.43391320e-07, -1.50090905e-01,  6.34592712e+00])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV-gVxJSk7Du",
        "outputId": "006893ab-23c1-4931-a831-92c115a974d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "110.83223565656992"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MSE(y,xscaled.dot(coef))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1_aHPuSgvPm"
      },
      "source": [
        "## SCAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "SLDy5ApX4eTE"
      },
      "outputs": [],
      "source": [
        "def scad_penalty(beta_hat, lambda_val, a_val):\n",
        "    is_linear = (np.abs(beta_hat) <= lambda_val)\n",
        "    is_quadratic = np.logical_and(lambda_val < np.abs(beta_hat), np.abs(beta_hat) <= a_val * lambda_val)\n",
        "    is_constant = (a_val * lambda_val) < np.abs(beta_hat)\n",
        "    \n",
        "    linear_part = lambda_val * np.abs(beta_hat) * is_linear\n",
        "    quadratic_part = (2 * a_val * lambda_val * np.abs(beta_hat) - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic\n",
        "    constant_part = (lambda_val**2 * (a_val + 1)) / 2 * is_constant\n",
        "    return linear_part + quadratic_part + constant_part\n",
        "    \n",
        "def scad_derivative(beta_hat, lambda_val, a_val):\n",
        "    return lambda_val * ((beta_hat <= lambda_val) + (a_val * lambda_val - beta_hat)*((a_val * lambda_val - beta_hat) > 0) / ((a_val - 1) * lambda_val) * (beta_hat > lambda_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "leB8eYOVhf0s"
      },
      "outputs": [],
      "source": [
        "X = xscaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "mvLvdV3ol7UN"
      },
      "outputs": [],
      "source": [
        "def scad(beta):\n",
        "  n = len(y)\n",
        "  return 1/n*np.sum((y-X.dot(beta))**2) + np.sum(scad_penalty(beta,lam,a))\n",
        "  \n",
        "def dscad(beta):\n",
        "  n = len(y)\n",
        "  return (-2/n)*(y-X.dot(beta)).dot(X)+scad_derivative(beta,lam,a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "O5_N9Ex7l8tf"
      },
      "outputs": [],
      "source": [
        "p = xscaled.shape[1] # p is the number of variables\n",
        "b0 = [0]*p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBnI_5ffp-1Y",
        "outputId": "b2a78c2c-2011-4de8-b794-af2bef6e0d74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8,)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(y- X.dot(b0+1)).dot(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qeKXpcVqTxF",
        "outputId": "1096ea11-e0fa-4d4a-9eab-95be29b895ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 1030)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.transpose(X).dot(y-X.dot(b0.reshape(-1,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "Wun-o5Hol8-t"
      },
      "outputs": [],
      "source": [
        "lam = 0.1\n",
        "a = 2\n",
        "output = minimize(scad, b0, method='L-BFGS-B', jac=dscad,options={'gtol': 1e-8, 'maxiter': 50000,'maxls': 25,'disp': True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCf9l0lAj2u4",
        "outputId": "66c4c271-1af5-4829-8a22-956321452fbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([12.26373913,  8.71309482,  5.42239662, -3.54868416,  1.61798777,\n",
              "        1.1580959 ,  1.33759283,  7.21553041])"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWsRmQIW0Yt7",
        "outputId": "e0dac6ab-09d4-4105-d4b8-468829f54c7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "107.21468778869392"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coef = output.x\n",
        "MSE(y,xscaled.dot(coef))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vToOAmV40ZMU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "DATA 410 Lecture 19 - Spring 2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
