{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import lstsq\n",
    "from scipy.interpolate import interp1d, LinearNDInterpolator, NearestNDInterpolator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb \n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tricubic Kernel, Lowess, and general Boosting definitions\n",
    "\n",
    "def Tricubic(x):\n",
    "  if len(x.shape) == 1:\n",
    "    x = x.reshape(-1,1)\n",
    "  d = np.sqrt(np.sum(x**2,axis=1))\n",
    "  return np.where(d>1,0,70/81*(1-d**3)**3)\n",
    "\n",
    "def lw_reg(X, y, xnew, kern, tau, intercept):\n",
    "    # tau is called bandwidth K((x-x[i])/(2*tau))\n",
    "    n = len(X) # the number of observations\n",
    "    yest = np.zeros(n)\n",
    "\n",
    "    if len(y.shape)==1: # here we make column vectors\n",
    "      y = y.reshape(-1,1)\n",
    "    if len(X.shape)==1:\n",
    "      X = X.reshape(-1,1)\n",
    "    if intercept:\n",
    "      X1 = np.column_stack([np.ones((len(X),1)),X])\n",
    "    else:\n",
    "      X1 = X\n",
    "\n",
    "    w = np.array([kern((X - X[i])/(2*tau)) for i in range(n)]) # here we compute n vectors of weights\n",
    "\n",
    "    #Looping through all X-points, solving for the predictions as linear combinations of inputs and weights matrix\n",
    "    for i in range(n):          \n",
    "        W = np.diag(w[:,i])\n",
    "        b = np.transpose(X1).dot(W).dot(y)\n",
    "        A = np.transpose(X1).dot(W).dot(X1)\n",
    "        #A = A + 0.001*np.eye(X1.shape[1]) # if we want L2 regularization\n",
    "        #theta = linalg.solve(A, b) # A*theta = b\n",
    "        beta, res, rnk, s = lstsq(A, b)\n",
    "        yest[i] = np.dot(X1[i],beta)\n",
    "    if X.shape[1]==1:\n",
    "      f = interp1d(X.flatten(),yest,fill_value='extrapolate')\n",
    "    else:\n",
    "      f = LinearNDInterpolator(X, yest)\n",
    "    output = f(xnew) # the output may have NaN's where the data points from xnew are outside the convex hull of X\n",
    "    if sum(np.isnan(output))>0:\n",
    "      g = NearestNDInterpolator(X,y.ravel()) \n",
    "      # output[np.isnan(output)] = g(X[np.isnan(output)])\n",
    "      output[np.isnan(output)] = g(xnew[np.isnan(output)])\n",
    "    return output  \n",
    "\n",
    "def rep_boosted_lwr(X, y, xtest, kern, tau, booster, nboost, intercept):\n",
    "  yhat = lw_reg(X,y,X,kern,tau,intercept) #get loess predictions on training data\n",
    "  yhat_test = lw_reg(X,y,xtest,kern,tau,intercept) #get loess predictions on testing data\n",
    "  lw_error = y - yhat #find the loess training residuals; these are what the booster will train on\n",
    "  #Below, fit the booster on train data and residuals, then add its predictions to the train/test predictions, then get new residuals\n",
    "  for i in range(nboost): \n",
    "    booster.fit(X, lw_error)\n",
    "    yhat += booster.predict(X)\n",
    "    yhat_test += booster.predict(xtest)\n",
    "    lw_error = y - yhat\n",
    "  return yhat_test\n",
    "\n",
    "def n_boost(X, y, xtest, model, nboost, booster, kern = None, tau = None, tau_b = None, \n",
    "            intercept = None, n_estimators=None , max_depth=None, model_nn = None):\n",
    "  if booster == 'LWR':\n",
    "    if model == 'LWR':\n",
    "      yhat = lw_reg(X,y,X,kern,tau,intercept) #get loess predictions on training data\n",
    "      yhat_test = lw_reg(X,y,xtest,kern,tau,intercept) #get loess predictions on testing data\n",
    "      lw_error = y - yhat #find the loess training residuals; these are what the booster will train on\n",
    "      for i in range(nboost): \n",
    "        yhat += lw_reg(X,lw_error,X,kern,tau_b,intercept)\n",
    "        yhat_test += lw_reg(X,lw_error,xtest,kern,tau_b,intercept)\n",
    "        lw_error = y - yhat\n",
    "      return yhat_test\n",
    "\n",
    "    if model == 'RF' or model == 'RFR':\n",
    "      model_rf = RandomForestRegressor(n_estimators=n_estimators,max_depth=max_depth)\n",
    "      model_rf.fit(X,y)\n",
    "      yhat_rf = model_rf.predict(X)\n",
    "      yhat_test = model_rf.predict(xtest)\n",
    "      rf_error = y - yhat_rf\n",
    "      for i in range(nboost): \n",
    "        yhat_rf += lw_reg(X,rf_error,X,kern,tau_b,intercept)\n",
    "        yhat_test += lw_reg(X,rf_error,xtest,kern,tau_b,intercept)\n",
    "        rf_error = y - yhat_rf\n",
    "      return yhat_test\n",
    "\n",
    "    if model == 'NN':\n",
    "      model_nn.fit(X,y,validation_split=0.3, epochs=100, batch_size=20, verbose=0, callbacks=[es])\n",
    "      yhat_nn = model_nn.predict(X)\n",
    "      yhat_test = model_nn.predict(xtest)\n",
    "      nn_error = y - yhat_nn\n",
    "      for i in range(nboost): \n",
    "        yhat_nn += lw_reg(X,nn_error,X,kern,tau_b,intercept)\n",
    "        yhat_test += lw_reg(X,nn_error,xtest,kern,tau_b,intercept)\n",
    "        nn_error = y-yhat_nn\n",
    "      return yhat_test\n",
    "\n",
    "  else:\n",
    "    if model == 'LWR':\n",
    "      yhat = lw_reg(X,y,X,kern,tau,intercept) #get loess predictions on training data\n",
    "      yhat_test = lw_reg(X,y,xtest,kern,tau,intercept) #get loess predictions on testing data\n",
    "      lw_error = y - yhat #find the loess training residuals; these are what the booster will train on\n",
    "      for i in range(nboost): \n",
    "        booster.fit(X, lw_error)\n",
    "        yhat += booster.predict(X)\n",
    "        yhat_test += booster.predict(xtest)\n",
    "        lw_error = y - yhat\n",
    "      return yhat_test\n",
    "\n",
    "    if model == 'RF' or model == 'RFR':\n",
    "      model_rf = RandomForestRegressor(n_estimators=n_estimators,max_depth=max_depth)\n",
    "      model_rf.fit(X,y)\n",
    "      yhat_rf = model_rf.predict(X)\n",
    "      yhat_test = model_rf.predict(xtest)\n",
    "      rf_error = y - yhat_rf\n",
    "      for i in range(nboost): \n",
    "        booster.fit(X, rf_error)\n",
    "        yhat_rf += booster.predict(X)\n",
    "        yhat_test += booster.predict(xtest)\n",
    "        rf_error = y - yhat_rf\n",
    "      return yhat_test\n",
    "\n",
    "    if model == 'NN':\n",
    "      model_nn.fit(X,y,validation_split=0.3, epochs=100, batch_size=20, verbose=0, callbacks=[es])\n",
    "      yhat_nn = model_nn.predict(X)\n",
    "      yhat_test = model_nn.predict(xtest)\n",
    "      nn_error = y - yhat_nn\n",
    "      for i in range(nboost): \n",
    "        booster.fit(X, nn_error)\n",
    "        yhat_nn += booster.predict(X)\n",
    "        yhat_test += booster.predict(xtest)\n",
    "        nn_error = y - yhat_nn\n",
    "      return yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Neural Network and XGB Architectures\\nmodel_nn = Sequential() #Making a tensorflow sequential network\\nmodel_nn.add(Dense(128, activation=\"relu\", input_dim=6))\\nmodel_nn.add(Dropout(0.1))\\nmodel_nn.add(Dense(64, activation=\"relu\"))\\nmodel_nn.add(Dropout(0.1))\\nmodel_nn.add(Dense(1, activation=\"linear\"))\\nmodel_nn.compile(loss=\\'mean_squared_error\\', optimizer=Adam(learning_rate= 0.001, decay = 0.0001))\\nes = EarlyStopping(monitor=\\'val_loss\\', mode=\\'min\\', verbose=1, patience=600)\\n\\nbooster_nn = Sequential()\\nbooster_nn.add(Dense(32, activation=\"relu\", input_dim=6))\\nbooster_nn.add(Dropout(0.2))\\nbooster_nn.add(Dense(16, activation=\"relu\"))\\nbooster_nn.add(Dropout(0.2))\\nbooster_nn.add(Dense(1, activation=\"linear\"))\\nbooster_nn.compile(loss=\\'mean_squared_error\\', optimizer=Adam(learning_rate= 0.001, decay = 0.0001))\\nes = EarlyStopping(monitor=\\'val_loss\\', mode=\\'min\\', verbose=1, patience=600)\\n\\nmodel_xgb = xgb.XGBRegressor(objective =\\'reg:squarederror\\',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Neural Network and XGB Architectures\n",
    "model_nn = Sequential() #Making a tensorflow sequential network\n",
    "model_nn.add(Dense(128, activation=\"relu\", input_dim=6))\n",
    "model_nn.add(Dropout(0.1))\n",
    "model_nn.add(Dense(64, activation=\"relu\"))\n",
    "model_nn.add(Dropout(0.1))\n",
    "model_nn.add(Dense(1, activation=\"linear\"))\n",
    "model_nn.compile(loss='mean_squared_error', optimizer=Adam(learning_rate= 0.001, decay = 0.0001))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=600)\n",
    "\n",
    "booster_nn = Sequential()\n",
    "booster_nn.add(Dense(32, activation=\"relu\", input_dim=6))\n",
    "booster_nn.add(Dropout(0.2))\n",
    "booster_nn.add(Dense(16, activation=\"relu\"))\n",
    "booster_nn.add(Dropout(0.2))\n",
    "booster_nn.add(Dense(1, activation=\"linear\"))\n",
    "booster_nn.compile(loss='mean_squared_error', optimizer=Adam(learning_rate= 0.001, decay = 0.0001))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=600)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data/Concrete_Data.csv\")\n",
    "data = np.concatenate([data[data.columns[0:6]].values, data[data.columns[-1]].values.reshape(-1,1)], axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 12345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nelkshake\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 260.501\n",
      "[2]\tvalid_0's l2: 235.115\n",
      "[3]\tvalid_0's l2: 214.276\n",
      "[4]\tvalid_0's l2: 196.958\n",
      "[5]\tvalid_0's l2: 183.648\n",
      "[6]\tvalid_0's l2: 172.04\n",
      "[7]\tvalid_0's l2: 163.271\n",
      "[8]\tvalid_0's l2: 155.262\n",
      "[9]\tvalid_0's l2: 147.55\n",
      "[10]\tvalid_0's l2: 142.83\n",
      "[11]\tvalid_0's l2: 138.304\n",
      "[12]\tvalid_0's l2: 134.547\n",
      "[13]\tvalid_0's l2: 131.652\n",
      "[14]\tvalid_0's l2: 128.582\n",
      "[15]\tvalid_0's l2: 126.265\n",
      "[16]\tvalid_0's l2: 125.554\n",
      "[17]\tvalid_0's l2: 124.069\n",
      "[18]\tvalid_0's l2: 122.762\n",
      "[19]\tvalid_0's l2: 122.24\n",
      "[20]\tvalid_0's l2: 121.835\n",
      "[21]\tvalid_0's l2: 121.715\n",
      "[22]\tvalid_0's l2: 121.167\n",
      "[23]\tvalid_0's l2: 121.343\n",
      "[24]\tvalid_0's l2: 121.255\n",
      "[25]\tvalid_0's l2: 120.937\n",
      "[26]\tvalid_0's l2: 120.863\n",
      "[27]\tvalid_0's l2: 121.183\n",
      "[28]\tvalid_0's l2: 121.615\n",
      "[29]\tvalid_0's l2: 121.503\n",
      "[30]\tvalid_0's l2: 121.425\n",
      "[31]\tvalid_0's l2: 121.692\n",
      "[32]\tvalid_0's l2: 122.013\n",
      "[33]\tvalid_0's l2: 122.342\n",
      "[34]\tvalid_0's l2: 122.504\n",
      "[35]\tvalid_0's l2: 122.78\n",
      "[36]\tvalid_0's l2: 123.505\n",
      "[37]\tvalid_0's l2: 123.976\n",
      "[38]\tvalid_0's l2: 124.278\n",
      "[39]\tvalid_0's l2: 124.96\n",
      "[40]\tvalid_0's l2: 125.182\n",
      "[41]\tvalid_0's l2: 125.756\n",
      "[42]\tvalid_0's l2: 125.909\n",
      "[43]\tvalid_0's l2: 126.385\n",
      "[44]\tvalid_0's l2: 126.542\n",
      "[45]\tvalid_0's l2: 126.868\n",
      "[46]\tvalid_0's l2: 126.861\n",
      "[47]\tvalid_0's l2: 127.364\n",
      "[48]\tvalid_0's l2: 127.671\n",
      "[49]\tvalid_0's l2: 128.078\n",
      "[50]\tvalid_0's l2: 128.404\n",
      "[51]\tvalid_0's l2: 128.798\n",
      "[52]\tvalid_0's l2: 129.001\n",
      "[53]\tvalid_0's l2: 129.508\n",
      "[54]\tvalid_0's l2: 129.559\n",
      "[55]\tvalid_0's l2: 129.837\n",
      "[56]\tvalid_0's l2: 130.484\n",
      "[57]\tvalid_0's l2: 130.8\n",
      "[58]\tvalid_0's l2: 131.13\n",
      "[59]\tvalid_0's l2: 131.435\n",
      "[60]\tvalid_0's l2: 131.638\n",
      "[61]\tvalid_0's l2: 131.976\n",
      "[62]\tvalid_0's l2: 132.038\n",
      "[63]\tvalid_0's l2: 132.258\n",
      "[64]\tvalid_0's l2: 132.481\n",
      "[65]\tvalid_0's l2: 132.576\n",
      "[66]\tvalid_0's l2: 132.697\n",
      "[67]\tvalid_0's l2: 133.036\n",
      "[68]\tvalid_0's l2: 133.395\n",
      "[69]\tvalid_0's l2: 133.521\n",
      "[70]\tvalid_0's l2: 133.7\n",
      "[71]\tvalid_0's l2: 134.013\n",
      "[72]\tvalid_0's l2: 134.175\n",
      "[73]\tvalid_0's l2: 134.267\n",
      "[74]\tvalid_0's l2: 134.399\n",
      "[75]\tvalid_0's l2: 134.643\n",
      "[76]\tvalid_0's l2: 134.975\n",
      "[77]\tvalid_0's l2: 135.142\n",
      "[78]\tvalid_0's l2: 135.441\n",
      "[79]\tvalid_0's l2: 135.704\n",
      "[80]\tvalid_0's l2: 135.945\n",
      "[81]\tvalid_0's l2: 136.156\n",
      "[82]\tvalid_0's l2: 136.296\n",
      "[83]\tvalid_0's l2: 136.594\n",
      "[84]\tvalid_0's l2: 136.68\n",
      "[85]\tvalid_0's l2: 136.89\n",
      "[86]\tvalid_0's l2: 137.056\n",
      "[87]\tvalid_0's l2: 137.306\n",
      "[88]\tvalid_0's l2: 137.528\n",
      "[89]\tvalid_0's l2: 137.763\n",
      "[90]\tvalid_0's l2: 138.006\n",
      "[91]\tvalid_0's l2: 138.208\n",
      "[92]\tvalid_0's l2: 138.387\n",
      "[93]\tvalid_0's l2: 138.557\n",
      "[94]\tvalid_0's l2: 138.741\n",
      "[95]\tvalid_0's l2: 139.06\n",
      "[96]\tvalid_0's l2: 139.113\n",
      "[97]\tvalid_0's l2: 139.277\n",
      "[98]\tvalid_0's l2: 139.369\n",
      "[99]\tvalid_0's l2: 139.499\n",
      "[100]\tvalid_0's l2: 139.573\n",
      "[101]\tvalid_0's l2: 139.815\n",
      "[102]\tvalid_0's l2: 139.934\n",
      "[103]\tvalid_0's l2: 140.178\n",
      "[104]\tvalid_0's l2: 140.222\n",
      "[105]\tvalid_0's l2: 140.499\n",
      "[106]\tvalid_0's l2: 140.793\n",
      "[107]\tvalid_0's l2: 140.954\n",
      "[108]\tvalid_0's l2: 141.143\n",
      "[109]\tvalid_0's l2: 141.198\n",
      "[110]\tvalid_0's l2: 141.449\n",
      "[111]\tvalid_0's l2: 141.484\n",
      "[112]\tvalid_0's l2: 141.554\n",
      "[113]\tvalid_0's l2: 141.589\n",
      "[114]\tvalid_0's l2: 141.776\n",
      "[115]\tvalid_0's l2: 141.884\n",
      "[116]\tvalid_0's l2: 142.065\n",
      "[117]\tvalid_0's l2: 142.17\n",
      "[118]\tvalid_0's l2: 142.356\n",
      "[119]\tvalid_0's l2: 142.513\n",
      "[120]\tvalid_0's l2: 142.695\n",
      "[121]\tvalid_0's l2: 142.967\n",
      "[122]\tvalid_0's l2: 143.011\n",
      "[123]\tvalid_0's l2: 143.183\n",
      "[124]\tvalid_0's l2: 143.392\n",
      "[125]\tvalid_0's l2: 143.602\n",
      "[126]\tvalid_0's l2: 143.736\n",
      "[127]\tvalid_0's l2: 144.063\n",
      "[128]\tvalid_0's l2: 144.351\n",
      "[129]\tvalid_0's l2: 144.553\n",
      "[130]\tvalid_0's l2: 144.692\n",
      "[131]\tvalid_0's l2: 144.73\n",
      "[132]\tvalid_0's l2: 144.862\n",
      "[133]\tvalid_0's l2: 144.967\n",
      "[134]\tvalid_0's l2: 145.168\n",
      "[135]\tvalid_0's l2: 145.342\n",
      "[136]\tvalid_0's l2: 145.583\n",
      "[137]\tvalid_0's l2: 145.65\n",
      "[138]\tvalid_0's l2: 145.843\n",
      "[139]\tvalid_0's l2: 146.033\n",
      "[140]\tvalid_0's l2: 146.167\n",
      "[141]\tvalid_0's l2: 146.344\n",
      "[142]\tvalid_0's l2: 146.431\n",
      "[143]\tvalid_0's l2: 146.579\n",
      "[144]\tvalid_0's l2: 146.744\n",
      "[145]\tvalid_0's l2: 146.871\n",
      "[146]\tvalid_0's l2: 146.979\n",
      "[147]\tvalid_0's l2: 147.111\n",
      "[148]\tvalid_0's l2: 147.17\n",
      "[149]\tvalid_0's l2: 147.278\n",
      "[150]\tvalid_0's l2: 147.501\n",
      "[151]\tvalid_0's l2: 147.71\n",
      "[152]\tvalid_0's l2: 147.781\n",
      "[153]\tvalid_0's l2: 147.875\n",
      "[154]\tvalid_0's l2: 147.948\n",
      "[155]\tvalid_0's l2: 148.054\n",
      "[156]\tvalid_0's l2: 148.143\n",
      "[157]\tvalid_0's l2: 148.316\n",
      "[158]\tvalid_0's l2: 148.418\n",
      "[159]\tvalid_0's l2: 148.464\n",
      "[160]\tvalid_0's l2: 148.527\n",
      "[161]\tvalid_0's l2: 148.538\n",
      "[162]\tvalid_0's l2: 148.654\n",
      "[163]\tvalid_0's l2: 148.734\n",
      "[164]\tvalid_0's l2: 148.813\n",
      "[165]\tvalid_0's l2: 148.968\n",
      "[166]\tvalid_0's l2: 149.18\n",
      "[167]\tvalid_0's l2: 149.225\n",
      "[168]\tvalid_0's l2: 149.366\n",
      "[169]\tvalid_0's l2: 149.526\n",
      "[170]\tvalid_0's l2: 149.622\n",
      "[171]\tvalid_0's l2: 149.7\n",
      "[172]\tvalid_0's l2: 149.803\n",
      "[173]\tvalid_0's l2: 149.921\n",
      "[174]\tvalid_0's l2: 149.975\n",
      "[175]\tvalid_0's l2: 150.079\n",
      "[176]\tvalid_0's l2: 150.199\n",
      "[177]\tvalid_0's l2: 150.315\n",
      "[178]\tvalid_0's l2: 150.368\n",
      "[179]\tvalid_0's l2: 150.464\n",
      "[180]\tvalid_0's l2: 150.561\n",
      "[181]\tvalid_0's l2: 150.698\n",
      "[182]\tvalid_0's l2: 150.779\n",
      "[183]\tvalid_0's l2: 150.916\n",
      "[184]\tvalid_0's l2: 151.039\n",
      "[185]\tvalid_0's l2: 151.007\n",
      "[186]\tvalid_0's l2: 151.159\n",
      "[187]\tvalid_0's l2: 151.27\n",
      "[188]\tvalid_0's l2: 151.423\n",
      "[189]\tvalid_0's l2: 151.537\n",
      "[190]\tvalid_0's l2: 151.654\n",
      "[191]\tvalid_0's l2: 151.728\n",
      "[192]\tvalid_0's l2: 151.842\n",
      "[193]\tvalid_0's l2: 151.895\n",
      "[194]\tvalid_0's l2: 151.903\n",
      "[195]\tvalid_0's l2: 152.003\n",
      "[196]\tvalid_0's l2: 152.084\n",
      "[197]\tvalid_0's l2: 152.181\n",
      "[198]\tvalid_0's l2: 152.291\n",
      "[199]\tvalid_0's l2: 152.419\n",
      "[200]\tvalid_0's l2: 152.487\n",
      "5.19Minutes in Split Number: 1\n"
     ]
    }
   ],
   "source": [
    "#data Cross-Validation\n",
    "mse_lwr_d = []\n",
    "mse_lwr_rf = []\n",
    "mse_rf_d = []\n",
    "mse_lwr_lwr = []\n",
    "mse_lgbm = []\n",
    "\n",
    "scale = StandardScaler()\n",
    "for i in range(12345,12346):\n",
    "  print('Random State: ' + str(i))\n",
    "  kf = KFold(n_splits=10,shuffle=True,random_state=i)\n",
    "  # this is the random state cross-validation loop to make sure our results are real, not just the state being good/bad for a particular model\n",
    "  j = 0\n",
    "  for idxtrain, idxtest in kf.split(data[:,:2]):\n",
    "    t = time.time()\n",
    "    j += 1\n",
    "    #Split the train and test data\n",
    "    xtrain = data[:,:6][idxtrain]\n",
    "    ytrain = data[:,-1][idxtrain]\n",
    "    ytest = data[:,-1][idxtest]\n",
    "    xtest = data[:,:6][idxtest]\n",
    "    xtrain = scale.fit_transform(xtrain)\n",
    "    xtest = scale.transform(xtest)\n",
    "\n",
    "\n",
    "    #LWR boosted with decision tree\n",
    "    booster = DecisionTreeRegressor(max_depth=2)\n",
    "    yhat_lwr_d = n_boost(xtrain, ytrain, xtest, model = 'LWR', nboost=3, booster=booster, \n",
    "                   kern = Tricubic, tau = 1.2, intercept = True,)\n",
    "    \n",
    "    #LWR boosted with RF\n",
    "    booster = RandomForestRegressor(n_estimators=25, max_depth=2)\n",
    "    yhat_lwr_rf = n_boost(xtrain, ytrain, xtest, model = 'LWR', nboost=3, booster=booster, \n",
    "                   kern = Tricubic, tau = 1.2, intercept = True)\n",
    "\n",
    "    #LWR boosted with LWR\n",
    "    booster='LWR'\n",
    "    yhat_lwr_lwr = n_boost(xtrain, ytrain, xtest, model = 'LWR', nboost=3, booster=booster, \n",
    "                   kern = Tricubic, tau = 1.2, tau_b=0.5, intercept = True)\n",
    "    \n",
    "    #RF boosted with decision tree\n",
    "    booster = DecisionTreeRegressor(max_depth=2)\n",
    "    yhat_rf_d = n_boost(xtrain, ytrain, xtest, model = 'RFR', nboost=3, booster=booster, n_estimators=100 , max_depth=3)\n",
    "    \n",
    "    #RF boosted with LWR\n",
    "    booster = 'LWR'\n",
    "    yhat_rf_d = n_boost(xtrain, ytrain, xtest, model = 'RFR', nboost=3, booster=booster, \n",
    "                   kern = Tricubic, tau = 1.2, tau_b=0.5, intercept = True, n_estimators=100 , max_depth=3)\n",
    "\n",
    "    #LightGBM\n",
    "    lgb = lgbm.LGBMRegressor(num_iterations=200)\n",
    "    lgb.fit(xtrain, ytrain, eval_set=[(xtest, ytest)], eval_metric='mse')\n",
    "    yhat_lgbm = lgb.predict(xtest, num_iteration=lgb.best_iteration_)\n",
    "\n",
    "\n",
    "    #Append each model's MSE\n",
    "    mse_lwr_lwr.append(mse(ytest,yhat_lwr_lwr))\n",
    "    mse_lwr_d.append(mse(ytest,yhat_lwr_d))\n",
    "    mse_lwr_rf.append(mse(ytest,yhat_lwr_rf))\n",
    "    mse_rf_d.append(mse(ytest,yhat_rf_d))\n",
    "    mse_lgbm.append(mse(ytest,yhat_lgbm))\n",
    "\n",
    "    dt = time.time() - t\n",
    "    print(str(np.around(dt/60, 2)) + 'Minutes in Split Number: ' + str(j))\n",
    "    \n",
    "\n",
    "print('\\n The Results for the Concrete Compressive Strength Dataset were:')\n",
    "print('The Cross-validated Mean Squared Error for LWR with Decision Tree is : '+str(np.mean(mse_lwr_d)))\n",
    "print('The Cross-validated Mean Squared Error for LWR with Random Forest is : '+str(np.mean(mse_lwr_rf)))\n",
    "print('The Cross-validated Mean Squared Error for Random Forest with Decision Tree is : '+str(np.mean(mse_rf_d)))\n",
    "print('The Cross-validated Mean Squared Error for LWR with LWR : '+str(np.mean(mse_lwr_lwr)))\n",
    "print('The Cross-validated Mean Squared Error for LightGBM : '+str(np.mean(mse_lgbm)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb8b7f83ee360ab7c65e3bf584de3c309f966a927bbfa0413b48c366d5d9edbf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
