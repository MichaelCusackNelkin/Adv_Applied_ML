{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import lstsq\n",
    "from scipy.interpolate import interp1d, LinearNDInterpolator, NearestNDInterpolator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb \n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tricubic Kernel, Lowess, and general Boosting definitions\n",
    "\n",
    "def Tricubic(x):\n",
    "  if len(x.shape) == 1:\n",
    "    x = x.reshape(-1,1)\n",
    "  d = np.sqrt(np.sum(x**2,axis=1))\n",
    "  return np.where(d>1,0,70/81*(1-d**3)**3)\n",
    "\n",
    "def lw_reg(X, y, xnew, kern, tau, intercept):\n",
    "    # tau is called bandwidth K((x-x[i])/(2*tau))\n",
    "    n = len(X) # the number of observations\n",
    "    yest = np.zeros(n)\n",
    "\n",
    "    if len(y.shape)==1: # here we make column vectors\n",
    "      y = y.reshape(-1,1)\n",
    "    if len(X.shape)==1:\n",
    "      X = X.reshape(-1,1)\n",
    "    if intercept:\n",
    "      X1 = np.column_stack([np.ones((len(X),1)),X])\n",
    "    else:\n",
    "      X1 = X\n",
    "\n",
    "    w = np.array([kern((X - X[i])/(2*tau)) for i in range(n)]) # here we compute n vectors of weights\n",
    "\n",
    "    #Looping through all X-points, solving for the predictions as linear combinations of inputs and weights matrix\n",
    "    for i in range(n):          \n",
    "        W = np.diag(w[:,i])\n",
    "        b = np.transpose(X1).dot(W).dot(y)\n",
    "        A = np.transpose(X1).dot(W).dot(X1)\n",
    "        #A = A + 0.001*np.eye(X1.shape[1]) # if we want L2 regularization\n",
    "        #theta = linalg.solve(A, b) # A*theta = b\n",
    "        beta, res, rnk, s = lstsq(A, b)\n",
    "        yest[i] = np.dot(X1[i],beta)\n",
    "    if X.shape[1]==1:\n",
    "      f = interp1d(X.flatten(),yest,fill_value='extrapolate')\n",
    "    else:\n",
    "      f = LinearNDInterpolator(X, yest)\n",
    "    output = f(xnew) # the output may have NaN's where the data points from xnew are outside the convex hull of X\n",
    "    if sum(np.isnan(output))>0:\n",
    "      g = NearestNDInterpolator(X,y.ravel()) \n",
    "      # output[np.isnan(output)] = g(X[np.isnan(output)])\n",
    "      output[np.isnan(output)] = g(xnew[np.isnan(output)])\n",
    "    return output  \n",
    "\n",
    "def rep_boosted_lwr(X, y, xtest, kern, tau, booster, nboost, intercept):\n",
    "  yhat = lw_reg(X,y,X,kern,tau,intercept) #get loess predictions on training data\n",
    "  yhat_test = lw_reg(X,y,xtest,kern,tau,intercept) #get loess predictions on testing data\n",
    "  lw_error = y - yhat #find the loess training residuals; these are what the booster will train on\n",
    "  #Below, fit the booster on train data and residuals, then add its predictions to the train/test predictions, then get new residuals\n",
    "  for i in range(nboost): \n",
    "    booster.fit(X, lw_error)\n",
    "    yhat += booster.predict(X)\n",
    "    yhat_test += booster.predict(xtest)\n",
    "    lw_error = y - yhat\n",
    "  return yhat_test\n",
    "\n",
    "def n_boost(X, y, xtest, model, nboost, booster, kern = None, tau = None, tau_b = None, \n",
    "            intercept = None, n_estimators=None , max_depth=None, model_nn = None):\n",
    "  if booster == 'LWR':\n",
    "    if model == 'LWR':\n",
    "      yhat = lw_reg(X,y,X,kern,tau,intercept) #get loess predictions on training data\n",
    "      yhat_test = lw_reg(X,y,xtest,kern,tau,intercept) #get loess predictions on testing data\n",
    "      lw_error = y - yhat #find the loess training residuals; these are what the booster will train on\n",
    "      for i in range(nboost): \n",
    "        yhat += lw_reg(X,lw_error,X,kern,tau_b,intercept)\n",
    "        yhat_test += lw_reg(X,lw_error,xtest,kern,tau_b,intercept)\n",
    "        lw_error = y - yhat\n",
    "      return yhat_test\n",
    "\n",
    "    if model == 'RF' or model == 'RFR':\n",
    "      model_rf = RandomForestRegressor(n_estimators=n_estimators,max_depth=max_depth)\n",
    "      model_rf.fit(X,y)\n",
    "      yhat_rf = model_rf.predict(X)\n",
    "      yhat_test = model_rf.predict(xtest)\n",
    "      rf_error = y - yhat_rf\n",
    "      for i in range(nboost): \n",
    "        yhat_rf += lw_reg(X,rf_error,X,kern,tau_b,intercept)\n",
    "        yhat_test += lw_reg(X,rf_error,xtest,kern,tau_b,intercept)\n",
    "        rf_error = y - yhat_rf\n",
    "      return yhat_test\n",
    "\n",
    "    if model == 'NN':\n",
    "      model_nn.fit(X,y,validation_split=0.3, epochs=100, batch_size=20, verbose=0, callbacks=[es])\n",
    "      yhat_nn = model_nn.predict(X)\n",
    "      yhat_test = model_nn.predict(xtest)\n",
    "      nn_error = y - yhat_nn\n",
    "      for i in range(nboost): \n",
    "        yhat_nn += lw_reg(X,nn_error,X,kern,tau_b,intercept)\n",
    "        yhat_test += lw_reg(X,nn_error,xtest,kern,tau_b,intercept)\n",
    "        nn_error = y-yhat_nn\n",
    "      return yhat_test\n",
    "\n",
    "  else:\n",
    "    if model == 'LWR':\n",
    "      yhat = lw_reg(X,y,X,kern,tau,intercept) #get loess predictions on training data\n",
    "      yhat_test = lw_reg(X,y,xtest,kern,tau,intercept) #get loess predictions on testing data\n",
    "      lw_error = y - yhat #find the loess training residuals; these are what the booster will train on\n",
    "      for i in range(nboost): \n",
    "        booster.fit(X, lw_error)\n",
    "        yhat += booster.predict(X)\n",
    "        yhat_test += booster.predict(xtest)\n",
    "        lw_error = y - yhat\n",
    "      return yhat_test\n",
    "\n",
    "    if model == 'RF' or model == 'RFR':\n",
    "      model_rf = RandomForestRegressor(n_estimators=n_estimators,max_depth=max_depth)\n",
    "      model_rf.fit(X,y)\n",
    "      yhat_rf = model_rf.predict(X)\n",
    "      yhat_test = model_rf.predict(xtest)\n",
    "      rf_error = y - yhat_rf\n",
    "      for i in range(nboost): \n",
    "        booster.fit(X, rf_error)\n",
    "        yhat_rf += booster.predict(X)\n",
    "        yhat_test += booster.predict(xtest)\n",
    "        rf_error = y - yhat_rf\n",
    "      return yhat_test\n",
    "\n",
    "    if model == 'NN':\n",
    "      model_nn.fit(X,y,validation_split=0.3, epochs=100, batch_size=20, verbose=0, callbacks=[es])\n",
    "      yhat_nn = model_nn.predict(X)\n",
    "      yhat_test = model_nn.predict(xtest)\n",
    "      nn_error = y - yhat_nn\n",
    "      for i in range(nboost): \n",
    "        booster.fit(X, nn_error)\n",
    "        yhat_nn += booster.predict(X)\n",
    "        yhat_test += booster.predict(xtest)\n",
    "        nn_error = y - yhat_nn\n",
    "      return yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network and XGB Architectures\n",
    "model_nn = Sequential() #Making a tensorflow sequential network\n",
    "model_nn.add(Dense(128, activation=\"relu\", input_dim=6))\n",
    "model_nn.add(Dropout(0.1))\n",
    "model_nn.add(Dense(64, activation=\"relu\"))\n",
    "model_nn.add(Dropout(0.1))\n",
    "model_nn.add(Dense(1, activation=\"linear\"))\n",
    "model_nn.compile(loss='mean_squared_error', optimizer=Adam(learning_rate= 0.001, decay = 0.0001))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=600)\n",
    "\n",
    "booster_nn = Sequential()\n",
    "booster_nn.add(Dense(32, activation=\"relu\", input_dim=6))\n",
    "booster_nn.add(Dropout(0.2))\n",
    "booster_nn.add(Dense(16, activation=\"relu\"))\n",
    "booster_nn.add(Dropout(0.2))\n",
    "booster_nn.add(Dense(1, activation=\"linear\"))\n",
    "booster_nn.compile(loss='mean_squared_error', optimizer=Adam(learning_rate= 0.001, decay = 0.0001))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=600)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data/Concrete_Data.csv\")\n",
    "data = np.concatenate([data[data.columns[0:6]].values, data[data.columns[-1]].values.reshape(-1,1)], axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 12345\n",
      "Split Number: 1\n",
      "Split Number: 2\n",
      "Split Number: 3\n",
      "Split Number: 4\n",
      "Split Number: 5\n",
      "Split Number: 6\n"
     ]
    }
   ],
   "source": [
    "#data Cross-Validation\n",
    "mse_lwr_d = []\n",
    "mse_lwr_rf = []\n",
    "mse_rf_d = []\n",
    "mse_lwr_lwr = []\n",
    "\n",
    "scale = StandardScaler()\n",
    "for i in range(12345,12346):\n",
    "  print('Random State: ' + str(i))\n",
    "  kf = KFold(n_splits=10,shuffle=True,random_state=i)\n",
    "  # this is the random state cross-validation loop to make sure our results are real, not just the state being good/bad for a particular model\n",
    "  j = 0\n",
    "  for idxtrain, idxtest in kf.split(data[:,:2]):\n",
    "    j += 1\n",
    "    #Split the train and test data\n",
    "    xtrain = data[:,:6][idxtrain]\n",
    "    ytrain = data[:,-1][idxtrain]\n",
    "    ytest = data[:,-1][idxtest]\n",
    "    xtest = data[:,:6][idxtest]\n",
    "    xtrain = scale.fit_transform(xtrain)\n",
    "    xtest = scale.transform(xtest)\n",
    "    print('Split Number: ' + str(j))\n",
    "\n",
    "    #LWR boosted with decision tree\n",
    "    booster = DecisionTreeRegressor(max_depth=2)\n",
    "    yhat_lwr_d = n_boost(xtrain, ytrain, xtest, model = 'LWR', nboost=3, booster=booster, \n",
    "                   kern = Tricubic, tau = 1.2, intercept = True,)\n",
    "    \n",
    "    #LWR boosted with RF\n",
    "    booster = RandomForestRegressor(n_estimators=25, max_depth=2)\n",
    "    yhat_lwr_rf = n_boost(xtrain, ytrain, xtest, model = 'LWR', nboost=3, booster=booster, \n",
    "                   kern = Tricubic, tau = 1.2, intercept = True)\n",
    "\n",
    "    #LWR boosted with LWR\n",
    "    booster='LWR'\n",
    "    yhat_lwr_lwr = n_boost(xtrain, ytrain, xtest, model = 'LWR', nboost=3, booster=booster, \n",
    "                   kern = Tricubic, tau = 1.2, tau_b=0.5, intercept = True)\n",
    "    \n",
    "    #RF boosted with decision tree\n",
    "    booster = DecisionTreeRegressor(max_depth=2)\n",
    "    yhat_rf_d = n_boost(xtrain, ytrain, xtest, model = 'RFR', nboost=3, booster=booster, n_estimators=100 , max_depth=3)\n",
    "    \n",
    "    #RF boosted with LWR\n",
    "    booster = 'LWR'\n",
    "    yhat_rf_d = n_boost(xtrain, ytrain, xtest, model = 'RFR', nboost=3, booster=booster, \n",
    "                   kern = Tricubic, tau = 1.2, tau_b=0.5, intercept = True, n_estimators=100 , max_depth=3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Append each model's MSE\n",
    "    mse_lwr_lwr.append(mse(ytest,yhat_lwr_lwr))\n",
    "    mse_lwr_d.append(mse(ytest,yhat_lwr_d))\n",
    "    mse_lwr_rf.append(mse(ytest,yhat_lwr_rf))\n",
    "    mse_rf_d.append(mse(ytest,yhat_rf_d))\n",
    "\n",
    "print('\\n The Results for the Concrete Compressive Strength Dataset were:')\n",
    "print('The Cross-validated Mean Squared Error for LWR with Decision Tree is : '+str(np.mean(mse_lwr_d)))\n",
    "print('The Cross-validated Mean Squared Error for LWR with Random Forest is : '+str(np.mean(mse_lwr_rf)))\n",
    "print('The Cross-validated Mean Squared Error for Random Forest with Decision Tree is : '+str(np.mean(mse_rf_d)))\n",
    "print('The Cross-validated Mean Squared Error for LWR with LWR : '+str(np.mean(mse_lwr_lwr)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "hyper_params = {\n",
    "\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': ['l1','l2'],\n",
    "    'learning_rate': 0.005,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 10,\n",
    "    'verbose': 0,\n",
    "    \"max_depth\": 8,\n",
    "    \"num_leaves\": 128,  \n",
    "    \"max_bin\": 512,\n",
    "    \"num_iterations\": 10000\n",
    "}\n",
    "\n",
    "lgbm = lgbm.LGBMRegressor(**hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(data[:,:6], data[:,-1], test_size=0.2, random_state=42)\n",
    "\n",
    "lgbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm.predict(X_train, num_iteration=gbm.best_iteration_)\n",
    "# Basic RMSE\n",
    "print('The rmse of prediction is:', round(mean_squared_log_error(y_pred, y_train) ** 0.5, 5))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb8b7f83ee360ab7c65e3bf584de3c309f966a927bbfa0413b48c366d5d9edbf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
